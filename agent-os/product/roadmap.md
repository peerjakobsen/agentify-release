# Product Roadmap

## Phase 1: Foundation (MVP)

1. [x] DynamoDB Observability Table â€” Create `agentify-workflow-events` DynamoDB table with workflow_id partition key, timestamp sort key, event_type, agent_name, payload, and TTL configuration `S`

2. [x] Agentify Extension Shell â€” Create single Kiro IDE extension with shared services (AWS clients, config, types) and registration for two webview panels: Demo Viewer (runtime) and Ideation Wizard (design-time) `S`

3. [x] AWS Credential Chain Integration â€” Use AWS SDK's default credential provider chain to automatically consume credentials from shared AWS config files (~/.aws/credentials, ~/.aws/config), supporting IAM credentials, IAM Identity Center (SSO), and assumed roles configured via AWS CLI or AWS Toolkit; add project-level region configuration in .agentify/config.json for DynamoDB and Bedrock API calls `S`

4. [x] Project Initialization Command â€” Add "Agentify: Initialize Project" command that: (1) checks AWS credentials via default credential chain, (2) validates DynamoDB table exists using tableValidator service, (3) if table missing, prompts user to deploy using bundled `infrastructure/dynamodb-table.yaml` template via CloudFormation SDK, (4) waits for stack CREATE_COMPLETE, (5) generates `.agentify/config.json` with table name, region, and stack name, (6) creates `.kiro/steering/agentify-integration.md` steering file. The CloudFormation template from spec #1 is packaged with the extension for automated deployment. `M`

5. [x] Workflow Input Panel â€” Build Demo Viewer input panel with: (1) multi-line prompt textarea (button submit only, no Enter shortcut), (2) Run Workflow button that spawns `agents/main.py` with `--prompt`, `--workflow-id`, and `--trace-id` CLI args, (3) hybrid identity display showing short `workflow_id` (wf-xxx) with copy button and OTEL `trace_id` with optional X-Ray console link, (4) execution timer, (5) validation states for missing entry script or AWS credentials. The `main.py` is generated by Kiro spec-driven development, not by this extension. `M`

6. [x] Execution Log Panel â€” Create chronological log panel displaying events from DynamoDB with timestamps, event types, agent names, and expandable payload details `M`

7. [x] Outcome Panel â€” Build outcome display section in Demo Viewer (below Execution Log) showing: (1) success/failure status with âœ…/âŒ icon from `workflow_complete` or `workflow_error` stdout events, (2) workflow result rendered as markdown when result is a string, with formatted JSON fallback (syntax highlighting, collapsible) for structured objects, (3) "Sources" line listing data sources used if provided in outcome payload, (4) copy-to-clipboard button for result content. Panel starts hidden/collapsed until first workflow completes, clears immediately when new run starts (not waiting for new outcome). Error state displays error message prominently without stack trace â€” keep it clean for demo audiences. Does NOT duplicate execution duration (already shown in Input Panel timer). `S`

8. [x] DynamoDB Polling Service â€” Implement polling service for workflow events: (1) poll `infrastructure.dynamodb.tableName` every 500ms using AWS SDK DocumentClient, (2) query by `workflow_id` (partition key) with `timestamp` (sort key) greater than last-polled timestamp to fetch only new events, (3) start polling when `handleRunWorkflow()` generates a workflow_id, (4) stop polling on `workflow_complete`/`workflow_error` event, panel dispose, or new workflow run, (5) exponential backoff on errors (1s, 2s, 4s, max 30s) with automatic recovery, (6) emit events to subscribers (for merging with stdout stream), (7) track seen event IDs for deduplication. Service is separate from panel lifecycle â€” panel subscribes to events, service manages AWS calls. `M`

9. [x] Observability Steering Documentation â€” Expand `.kiro/steering/agentify-integration.md` (created in item 4) with complete implementation guidance for Kiro-generated agent code:

**CLI Contract:**
- `agents/main.py` must accept `--prompt`, `--workflow-id`, `--trace-id` via argparse
- Read `AGENTIFY_TABLE_NAME` and `AGENTIFY_TABLE_REGION` environment variables
- Example argparse setup code snippet

**Hybrid Identity Pattern:**
- `workflow_id`: Short ID (wf-{8-char}) for UI display and DynamoDB partition key
- `trace_id`: 32-char hex OTEL trace ID for X-Ray correlation
- Both IDs included in every emitted event

**stdout Event Streaming (real-time, for graph visualization):**
- JSON Lines format (one JSON object per line to stdout)
- Event types: `graph_structure` (topology at start), `node_start`/`node_stream`/`node_stop` (agent lifecycle), `workflow_complete`/`workflow_error` (terminal)
- Schema: `{"workflow_id": "...", "trace_id": "...", "timestamp": 1234567890, "event_type": "...", "payload": {...}}`
- Example: mapping Strands `stream_async()` callbacks to stdout events

**DynamoDB Event Persistence (for tool calls and history):**
- Event types: `tool_call`, `tool_result`, `agent_start`, `agent_end`, `handoff`
- Write to table from `AGENTIFY_TABLE_NAME` env var using boto3
- Include TTL field for automatic cleanup
- Example: `emit_event()` helper function pattern

**Strands SDK Integration:**
- `StrandsTelemetry` setup for OTEL trace propagation
- `stream_async()` event callback mapping to stdout JSON lines
- Agent decorator patterns for consistent event emission

This steering file is the source of truth for Kiro's code generation â€” all generated agents must follow these patterns. `M`

10. [x] Workflow Trigger Service â€” Extract subprocess execution logic from `DemoViewerPanel.handleRunWorkflow()` into a dedicated service with clean separation:

**ID Generation:**
- `workflow_id`: `wf-{8-char-uuid}` (e.g., `wf-a1b2c3d4`)
- `trace_id`: 32-char hex string, OTEL-compatible (e.g., `80e1afed08e019fc1110464cfa66635c`)

**Subprocess Spawning:**
- Command: `{workflow.pythonPath} {workflow.entryScript}` from `.agentify/config.json`
- CLI args: `--prompt`, `--workflow-id`, `--trace-id`
- Env vars: `AGENTIFY_TABLE_NAME` (from `infrastructure.dynamodb.tableName`), `AGENTIFY_TABLE_REGION` (from `infrastructure.dynamodb.region`)
- Working directory: workspace root

**Event Emission (vscode.EventEmitter pattern):**
- `onStdoutLine: Event<string>` â€” raw stdout lines (item 11 parses these)
- `onStderr: Event<string>` â€” stderr output for error display
- `onProcessStateChange: Event<ProcessState>` â€” `idle` | `running` | `completed` | `failed` | `killed`
- `onProcessExit: Event<{code: number | null, signal: string | null}>`

**Process Lifecycle:**
- `start(prompt: string): {workflowId, traceId}` â€” spawns process, returns IDs
- `kill(): void` â€” terminates current process (for reset/new run)
- `getState(): ProcessState` â€” current process state
- Only one workflow at a time â€” calling `start()` while running kills previous process

**Integration:**
- Singleton service (like `DynamoDbPollingService`)
- `DemoViewerPanel.handleRunWorkflow()` calls this service instead of spawning directly
- Panel subscribes to events for UI updates

This service handles raw subprocess I/O; stdout JSON parsing is handled by item 11. `M`

11. [x] stdout Event Streaming & Panel Integration â€” Parse real-time JSON line events from `WorkflowTriggerService.onStdoutLine` following the schema in `agentify-integration.md`: `graph_structure`, `node_start`/`node_stream`/`node_stop`, `tool_call`/`tool_result`, `workflow_complete`/`workflow_error`. Create `StdoutEventParser` service that:

**Parsing:**
- Subscribes to `WorkflowTriggerService.onStdoutLine`
- Parses JSON, validates against event schema
- Emits typed `StdoutEvent` via `vscode.EventEmitter`
- Handles malformed JSON gracefully (log warning, skip)

**Panel Integration:**
- `DemoViewerPanel` subscribes to both `StdoutEventParser.onEvent` and `DynamoDbPollingService.onEvent`
- Events collected into single array, sorted by timestamp for Execution Log display
- `workflow_complete`/`workflow_error` events trigger Outcome Panel update
- `graph_structure`/`node_*` events reserved for Phase 4 Agent Graph visualization

No separate merge service needed â€” panel handles trivial array combination. `M`

## Phase 2: AI-Assisted Ideation

13. [x] Ideation Wizard Panel & Business Objective Step â€” Create Ideation Wizard webview panel with multi-step wizard navigation framework (step indicator, next/back buttons, progress tracking), then build the first step with: (1) multi-line text input for business objective/problem statement, (2) industry vertical dropdown (Retail, FSI, Healthcare, Life Sciences, Manufacturing, Energy, Telecom, Public Sector, Media & Entertainment, Travel & Hospitality, Other) with conditional "Other industry" free-text field when "Other" is selected, (3) system checkboxes grouped by category â€” CRM (Salesforce, HubSpot, Dynamics), ERP (SAP S/4HANA, Oracle, NetSuite), Data (Databricks, Snowflake, Redshift), HR (Workday, SuccessFactors), Service (ServiceNow, Zendesk), (4) "Other systems" free-text field, (5) optional file upload for additional context (account plan, requirements doc â€” stored in memory, not persisted). Wizard state held in memory; file persistence added in item 22. `M`

14. [x] Claude Bedrock Integration â€” Implement Amazon Bedrock client service for Claude API calls:

**Client Setup:**
- Use `@aws-sdk/client-bedrock-runtime` with credential chain from shared AWS services
- Model: `global.anthropic.claude-opus-4-5-20251101-v1:0` (configurable in `.agentify/config.json`)
- Region from `infrastructure.dynamodb.region` (same as DynamoDB)
- Use **Converse API** (`ConverseStreamCommand`) â€” the unified, model-agnostic interface

**Conversation Management:**
- `BedrockConversationService` singleton with `vscode.EventEmitter` pattern
- Maintain conversation history using Converse API message format:
```typescript
  interface Message {
    role: 'user' | 'assistant';
    content: Array<{ text: string }>;
  }
```
- System prompt loaded from bundled `prompts/ideation-assistant.md`, passed via `system` parameter
- `sendMessage(userMessage: string): AsyncIterable<string>` â€” streaming response
- `resetConversation(): void` â€” clear history for new ideation session

**Streaming to Webview:**
- Use `ConverseStreamCommand` from Converse API
- Async iterate `response.stream`, extract `contentBlockDelta.delta.text` tokens
- Emit tokens via `onToken: Event<string>`
- Emit `onComplete: Event<string>` with full response on `messageStop` event
- Handle `ThrottlingException` with exponential backoff (1s, 2s, 4s, max 30s)

**Error Handling:**
- `onError: Event<BedrockError>` for UI display
- Graceful handling of model access errors (user may not have Bedrock enabled)
- Handle `AccessDeniedException` for cross-region inference SCP issues `M`

15. [x] AI Gap-Filling Conversation â€” Create wizard step 2 as a conversational UI where the model analyzes the business objective and system selections, then proposes industry-typical assumptions:

**Conversation Flow:**
1. On step entry, auto-send context summary to Bedrock: "User's objective: {objective}. Industry: {industry}. Known systems: {systems}."
2. Model responds with structured proposal: "Based on your {industry} context, here's what I'm assuming about your environment..." with specific module/integration assumptions
3. User can accept all, or reply with corrections: "Actually we use SAP IBP, not APO"
4. Model acknowledges and refines: "Got it, updating to SAP IBP for demand planning..."
5. Conversation continues until user clicks "Confirm & Continue"

**UI Pattern:**
- Chat-style interface with AI messages (left-aligned) and user messages (right-aligned)
- AI messages include "Accept Assumptions" button for quick confirmation
- Editable text input for user refinements
- Streaming token display as model responds
- "Regenerate" button to get fresh proposal

**State Output:**
- `confirmedAssumptions: {system: string, modules: string[], integrations: string[]}[]`
- Stored in wizard state for downstream steps `L`

16. [x] Outcome Definition Step â€” Build wizard step 3 for defining measurable business outcomes:

**AI-Driven Suggestions on Step Entry:**
- Auto-send context (objective, industry, assumptions from Step 2) to Bedrock
- Model returns JSON with suggested primary outcome, KPIs, and relevant stakeholders
- Display suggestions as editable starting points, not locked values

**Fields:**
- Primary outcome statement (text input pre-filled with AI suggestion, fully editable)
- Success metrics (repeatable field group with AI-suggested KPIs, user can edit/add/remove)
- Stakeholders (multi-select with AI-suggested options pre-checked, user can uncheck/add custom)

**User Control:**
- All AI suggestions are editable, removable, or ignorable
- "Add Custom" option always available for each field
- "Regenerate Suggestions" button for fresh AI proposal
- User modifications take precedence over AI suggestions

**Fallback:**
- If AI fails: show static stakeholder list (Internal/External groups) and empty fields
- Manual entry always works regardless of AI status

**Validation:**
- Primary outcome required
- At least one success metric recommended (warning, not blocking) `S`

16.2. [x] Panel Architecture Consolidation â€” Consolidate duplicate ideation wizard implementations into single `tabbedPanel.ts`:

**Current State:**
- `src/panels/ideationWizardPanel.ts` (~3000 lines): Original standalone wizard with complete AI integration for Steps 1-3, including `OutcomeDefinitionService` integration, streaming handlers, and conversation management
- `src/panels/tabbedPanel.ts` (~2100 lines): Newer unified tabbed panel (Ideation + Demo Viewer) currently used by the app, with manual Step 3 implementation (no AI)

**Problem:**
- Two parallel implementations cause confusion during development
- AI integration code in `ideationWizardPanel.ts` is not being used
- Step 3 in `tabbedPanel.ts` lacks AI-driven suggestions that exist in `ideationWizardPanel.ts`

**Migration Tasks:**

1. **Port AI Integration for Step 3:**
   - Import and initialize `OutcomeDefinitionService` in `tabbedPanel.ts`
   - Add `_outcomeService` and `_outcomeStreamingResponse` private members
   - Implement `initOutcomeService()` method with event subscriptions
   - Implement `triggerAutoSendForStep3()` to auto-send context on step entry
   - Implement `sendOutcomeContextToClaude()` for AI suggestions
   - Add streaming handlers: `handleOutcomeStreamingToken()`, `handleOutcomeStreamingComplete()`, `handleOutcomeStreamingError()`

2. **Port Type Definitions:**
   - Verify `OutcomeSuggestions` interface from `wizardPanel.ts` is used
   - Ensure `parseOutcomeSuggestionsFromResponse()` from service is called

3. **Update Step Navigation:**
   - Add `triggerAutoSendForStep3()` call in `ideationNavigateForward()` when entering Step 3 from Step 2

4. **Port Prompt Template:**
   - Ensure `resources/prompts/outcome-definition-assistant.md` is loaded by service

5. **Delete Redundant Code:**
   - Remove `src/panels/ideationWizardPanel.ts` entirely
   - Remove `IDEATION_WIZARD_VIEW_ID` export and any package.json references
   - Clean up any unused imports in extension.ts

6. **Update Tests:**
   - Move relevant tests from `ideationWizardPanel.*.test.ts` to tabbedPanel tests
   - Delete `src/test/panels/ideationWizardPanel.*.test.ts` files

**Verification:**
- Step 3 auto-generates AI suggestions on entry (like Step 2 does)
- Regenerate button fetches fresh AI suggestions
- AI suggestions populate form fields (primary outcome, metrics, stakeholders)
- User edits are preserved and not overwritten by subsequent AI calls
- All existing Step 1-2 functionality unchanged `M`

16.5. [x] Outcome Refinement Conversation â€” Add conversational refinement UI to Step 3, matching the Step 2 pattern:

**Two-Phase Display:**
- **Phase 1 (Suggestion Review)**: On step entry, display AI suggestions as read-only card (not editable form)
  - Card shows: Primary Outcome statement, Suggested KPIs as bullet list, Suggested Stakeholders as tags
  - "Accept Suggestions" button (green, full-width) to transition to Phase 2
  - Refine input visible below card for pre-acceptance adjustments
- **Phase 2 (Editable Form)**: After acceptance, show current editable form (from item 16)
  - "Accepted âœ“" banner at top (matches Step 2 pattern)
  - All fields now editable (textarea, metric rows, stakeholder checkboxes)
  - Refine input remains visible for post-acceptance adjustments

**Refine Input (Both Phases):**
- Text input with placeholder: "Refine outcomes..."
- "Send" button to submit refinement request
- Example hints below input: "Add a metric for cost savings", "Make the outcome more specific to risk"
- Sends natural language request to Claude with current outcome state as context
- AI responds with updated suggestions (Phase 1) or directly updates form fields (Phase 2)

**Refinement Handling:**
- Parse AI response for structured changes: outcome text updates, KPI additions/removals/edits, stakeholder changes
- In Phase 1: Update suggestion card with refined values
- In Phase 2: Update form fields directly, preserve user's other manual edits
- Show brief "Updating..." indicator while AI processes refinement

**State Management:**
- New field: `suggestionsAccepted: boolean` (false on step entry, true after Accept click)
- Preserve `suggestionsAccepted: true` when navigating back and returning to Step 3
- "Regenerate" button resets to Phase 1 (`suggestionsAccepted: false`) with fresh AI call

**Conversation Context:**
- Refinement requests include: business objective, industry, confirmed assumptions (Step 2), current outcome state
- AI maintains context for multi-turn refinements within the step
- Conversation resets on "Regenerate" or when leaving and re-entering step with fresh data

**UI Consistency with Step 2:**
- Suggestion card styling matches Step 2 assumption cards (bordered container, system headers)
- Accept button matches Step 2 green "Accepted âœ“" style
- Refine input matches Step 2 "Refine assumptions..." input styling
- Loading and error states match Step 2 patterns `M`

17. [x] Security & Guardrails Step â€” Build wizard step 4 for compliance and approval gate configuration:

**Data Sensitivity Classification:**
- Radio buttons: Public, Internal, Confidential, Restricted
- Helper text explaining each level
- Default: Internal

**Compliance Frameworks (checkboxes):**
- SOC 2, HIPAA, PCI-DSS, GDPR, FedRAMP, None/Not specified
- Industry-aware defaults (Healthcare â†’ HIPAA pre-checked, FSI â†’ PCI-DSS + SOC 2)

**Human Approval Gates:**
- Checkbox list of workflow stages where human approval may be required
- Options: "Before external API calls", "Before data modification", "Before sending recommendations", "Before financial transactions"
- Default: None (fully automated demo)

**Guardrail Notes (optional text area):**
- Free-form notes for additional constraints
- On step entry: AI suggests relevant guardrail notes based on context from steps 1-2
- Suggestions are editable, user can modify or clear entirely
- Example placeholder if AI fails: "No PII in demo data, mask account numbers..."

This step is optional â€” "Skip" button available with sensible defaults applied. `S`

18. [x] Agent Design Proposal â€” Create wizard step 5 where model proposes agent team:

**State Structure:**
Add `agentDesign: AgentDesignState` to IdeationState following existing patterns:
```typescript
interface AgentDesignState {
  // AI Proposal
  proposedAgents: ProposedAgent[];
  proposedOrchestration: OrchestrationPattern;
  proposedEdges: ProposedEdge[];
  orchestrationReasoning: string;

  // Accept/Edit State
  proposalAccepted: boolean;
  isLoading: boolean;
  error?: string;

  // Change Detection
  step4Hash?: string;
  aiCalled: boolean;
}

interface ProposedAgent {
  id: string;
  name: string;
  role: string;
  tools: string[];  // AI-generated from Step 1 systems
}

interface ProposedEdge {
  from: string;
  to: string;
  condition?: string;  // For graph pattern
}
```

**Auto-Proposal on Step Entry:**
- Trigger: `triggerAutoSendForStep5()` following Step 3 pattern
- Change detection: Hash of Steps 1-4 inputs
- Send context to Bedrock, request JSON-structured agent team
- Parse response, populate `proposed*` fields

**Display (Phase 1 â€” Before Accept):**
- Card grid: each agent shows name, role, tools as tags
- Orchestration badge: "Graph" / "Swarm" / "Workflow"
- "Why this pattern?" expandable with `orchestrationReasoning`
- Text-based flow summary (NOT visual diagram):
```
  Flow: Planner â†’ Recommender â†’ Output
```

**Actions:**
- "â†» Regenerate" â€” `handleRegenerateAgentProposal()`, clears and re-fetches
- "Accept & Continue" â€” sets `proposalAccepted: true`, proceeds to Step 6
- "Let me adjust..." â€” sets `proposalAccepted: true`, stays on step, shows edit UI (item 19)

**Tool Generation:**
- AI generates tool names based on systems from Step 1
- Format: `{system}_{operation}` (e.g., `sap_get_inventory`, `salesforce_query_accounts`)
- Editable in item 19 `M`

19. [x] Agent Design Refinement â€” Enable editing when "Let me adjust..." selected:

**Transition:**
- Same page, different UI mode (like Step 3's Phase 1 â†’ Phase 2)
- Show "Accepted âœ“" banner (following Step 3 pattern)

**Agent Card Editing:**
- Each agent as editable card with edited flags:
  - `nameEdited`, `roleEdited`, `toolsEdited` per agent
- Fields: Name (text), Role (textarea), Tools (tag input with Ã— remove)
- "Ã— Remove Agent" with confirmation if agent has edges
- "+ Add Agent" opens card with empty fields

**Orchestration Adjustment:**
- Dropdown: graph / swarm / workflow
- Shows AI recommendation badge on original suggestion
- On change: AI suggests updated edges (non-blocking suggestion)

**Edge Editing:**
- Simple table: "From" dropdown â†’ "To" dropdown
- Add/remove edge buttons
- Validation warnings (non-blocking):
  - Orphan agents (no connections)
  - No entry point

**AI Assistance (Optional â€” Deferred):**
- "âœ¨ Suggest tools" button per agent â†’ quick AI call
- "Validate Design" button â†’ AI reviews, shows suggestions in toast

**Confirm:**
- "Confirm Design" copies to `confirmed*` fields, proceeds to Step 6
- Edited flags prevent AI overwrite on back-navigation `L`

20. [x] Orchestration Pattern Help â€” Tooltips on dropdown options explaining each pattern:

**Implementation:**
- Added `title` attributes to orchestration dropdown `<option>` elements
- Hover over any option (graph/swarm/workflow) to see description

**Pattern Descriptions:**
- **graph**: "LLM picks path at runtime based on conditions. Best for: approval gates, decision trees, conditional workflows."
- **swarm**: "Agents hand off autonomously. Best for: complex problem-solving, collaborative analysis, emergent behavior."
- **workflow**: "Fixed DAG with parallel execution. Best for: predictable pipelines, batch processing, strict ordering."

**Deferred:**
- Edge condition labels (Graph-specific)
- Handoff limits (Swarm-specific)
- Parallel group config (Workflow-specific)

21. [x] Mock Data Strategy â€” Build wizard step 6 for AI-generated mock data configuration:

**Auto-Generation on Step Entry:**
- For each tool identified in agent design, model proposes mock data shape:
```json
{
  "tool": "sap_inventory",
  "system": "SAP S/4HANA",
  "operation": "get_stock_levels",
  "mockRequest": {"warehouse_id": "string", "sku_list": "string[]"},
  "mockResponse": {"sku": "string", "quantity": "number", "location": "string"},
  "sampleData": [
    {"sku": "TOMATO-001", "quantity": 150, "location": "Produce-A3"}
  ]
}
```

**Display & Editing:**
- Accordion for each tool with mock definition
- JSON editor for request/response schemas (with syntax highlighting)
- Sample data table with add/edit/delete rows
- "Use customer terminology" toggle: when ON, model regenerates with industry-specific naming from wizard context

**Bulk Actions:**
- "Regenerate All" â€” fresh mock data proposal from model
- "Import Sample Data" â€” upload CSV/JSON to populate sample data tables

**Validation:**
- Warn if any tool missing mock definition
- Warn if sample data empty (demo won't be realistic)

**Output:**
- Mock definitions stored in wizard state
- Used in Phase 4 steering file generation (`integration-landscape.md`) `M`

22. [x] Wizard State Persistence â€” Implement workspace storage for wizard progress so users can resume incomplete ideation sessions:

**Storage:**
- Save wizard state to `.agentify/wizard-state.json` on each step completion
- State includes: current step, all field values, conversation history, agent design, mock data config
- Exclude uploaded files (too large) â€” store file metadata only with "re-upload required" flag

**Resume Flow:**
- On Ideation Wizard open, check for existing `wizard-state.json`
- If found and less than 7 days old: prompt "Resume previous session?" with preview of business objective
- "Resume" â†’ restore state, navigate to last completed step
- "Start Fresh" â†’ delete state file, begin at step 1

**Auto-Save:**
- Debounced save (500ms after last change) within each step
- Explicit save on "Next" button click

**Clear State:**
- "Reset Wizard" command clears state file and restarts
- State automatically cleared when steering files successfully generated (Phase 4) `S`

23. [x] Demo Design Step â€” Build wizard step 7 for capturing demo presentation strategy:

**Key "Aha Moments" (repeatable field group):**
- Moment title: "What should impress the audience?" â€” e.g., "Real-time SAP inventory sync"
- When it occurs: dropdown selecting which agent/tool triggers this moment
- What to say: suggested talking point for presenter

**Demo Persona:**
- Persona name (text): e.g., "Maria, Regional Inventory Manager"
- Persona role (text): e.g., "Reviews morning replenishment recommendations for 12 stores"
- Persona pain point (text): e.g., "Currently spends 2 hours manually checking stock levels"
- AI assist: "Generate Persona" button creates persona based on industry/objective context

**Narrative Flow:**
- Ordered list of demo scenes (drag-to-reorder)
- Each scene: title, description, which agents are highlighted
- "Generate Narrative" button: model proposes scene sequence based on agent design

**Output:**
- Stored in wizard state for `demo-strategy.md` generation in Phase 4 `M`

24. [x] Generate Step (Wizard Step 8) â€” Build the final wizard step that orchestrates steering file generation and Kiro handoff:

**Pre-Generation Checklist Display:**
- Show read-only summary of all wizard inputs across steps 1-7
- Validation status for each step (green check if complete, warning if optional fields skipped)
- "Edit" button next to each section to jump back to that step

**Generation Progress UI:**
- Checklist with real-time status updates:
  - [ ] Validate wizard inputs
  - [ ] Generate steering files (â†’ Phase 4, Item 28)
  - [ ] Ready for Kiro
- Each item shows spinner while in progress, checkmark on success, X on failure
- Error details expandable if any step fails
- Note: Agentify Power is installed during project initialization, not here

**Actions:**
- "Generate" button â€” triggers the generation sequence
- "Generate & Open in Kiro" button â€” generates then triggers Kiro spec flow (â†’ Phase 4, Item 34)
- Progress is non-blocking â€” user can see what's happening

**Post-Generation:**
- Success state shows generated file list with "Open File" links
- "Start Over" button to begin new ideation session (clears wizard state)
- If not in Kiro IDE, show message directing user to open project in Kiro

**Dependencies:**
- Requires Phase 3 Item 28 (steering generation) and Item 34 (Kiro trigger) for full functionality
- Agentify Power (Items 29-33) is installed at project init, not here
- Can show placeholder/disabled state until Phase 3 is complete `M`

## Phase 3: Kiro Integration & Enforcement

**Implementation Context:**
- **Demo Scope:** This entire extension generates demos with mock integrations - no real system integrations (SAP, Salesforce, etc.)
- **Local Entry:** `main.py` runs locally, triggers the agent workflow
- **Agent Runtime:** Strands agents deploy to AgentCore Runtime via **AgentCore CLI**
- **Tools:** Either inline with agent code OR as Lambda functions (for shared tools across agents)
- **Orchestration:** Steering files describe the pattern (graph/swarm/workflow), agent definitions, edges, and entry point(s)

28.1. [x] Steering Document Prompts â€” Create system prompts for transforming wizard state into Kiro steering markdown:

**Prompt File Location:**
```
resources/prompts/steering/
â”œâ”€â”€ product-steering.prompt.md
â”œâ”€â”€ tech-steering.prompt.md
â”œâ”€â”€ structure-steering.prompt.md
â”œâ”€â”€ customer-context-steering.prompt.md
â”œâ”€â”€ integration-landscape-steering.prompt.md
â”œâ”€â”€ security-policies-steering.prompt.md
â”œâ”€â”€ demo-strategy-steering.prompt.md
â””â”€â”€ agentify-integration-steering.prompt.md
```

**Each Prompt Defines:**
- Expected markdown structure and sections for that document
- What the document is for (Kiro steering context)
- Formatting guidelines and examples
- JSON input schema it expects from wizard state

**State Mapping Per Document:**
| Document | Wizard State Sections Used |
|----------|---------------------------|
| `product.md` | businessObjective, industry, outcome (primaryOutcome, successMetrics, stakeholders) |
| `tech.md` | agentDesign (agents, orchestration, edges), securityGuardrails (for policy mapping) |
| `structure.md` | agentDesign.confirmedAgents (for folder names), mockData.mockDefinitions (for tools) |
| `customer-context.md` | industry, systems, aiGapFillingState.confirmedAssumptions |
| `integration-landscape.md` | systems, agentDesign (for shared tools analysis), mockData |
| `security-policies.md` | securityGuardrails (sensitivity, frameworks, approvalGates) |
| `demo-strategy.md` | demoStrategy (ahaMoments, persona, narrativeScenes) |
| `agentify-integration.md` | agentDesign.confirmedAgents (agent IDs), orchestration pattern |

**AgentCore Features Guidance (in tech-steering.prompt.md):**
Prompt should guide Kiro on which AgentCore features to use:
- **Runtime**: Deploy agents via AgentCore CLI (serverless, session isolation)
- **Gateway**: Register shared Lambda tools (auto-converts to MCP-compatible)
- **Policy**: Map Step 4 guardrails to Cedar policies (approval gates â†’ boundaries)
- **Observability**: Agentify DynamoDB events (powers Demo Viewer panel)
- **Memory/Identity/Evaluations**: Optional, note when useful

**Shared Tools Analysis (in integration-landscape-steering.prompt.md):**
Prompt should instruct model to:
- Identify duplicate tools across agents (e.g., multiple agents use `databricks_query`)
- Flag shared tools for Lambda deployment + Gateway registration
- Keep per-agent tools inline with agent code
- Output as markdown tables with "Used By" column

**agentify-integration.md Content:**
- Event emission contract (DynamoDB schema for workflow events)
- Agent IDs for OpenTelemetry trace correlation
- CLI invocation pattern for Demo Viewer (`python main.py --workflow-id {id}`)
- Required decorators/instrumentation patterns `M`

28.2. [x] Steering Generation Service â€” Implement service that generates steering files using prompts from Item 28.1:

**Service Interface:**
```typescript
// src/services/steeringGenerationService.ts
interface GenerationResult {
  success: boolean;
  files: GeneratedFile[];
  errors?: { file: string; error: string }[];
}

interface GeneratedFile {
  fileName: string;
  filePath: string;
  content: string;
  status: 'created' | 'failed';
}

class SteeringGenerationService {
  async generateSteeringFiles(state: IdeationState): Promise<GenerationResult>;
  async generateDocument(promptName: string, context: object): Promise<string>;
}
```

**Generation Flow:**
1. Load all prompt files from `resources/prompts/steering/`
2. Extract relevant state sections for each document (per mapping in 28.1)
3. Generate all 8 documents in parallel using Bedrock
4. Return results with content and status per file

**Per-Document Generation:**
```typescript
private async generateDocument(promptName: string, context: object): Promise<string> {
  const systemPrompt = await this.loadPrompt(`steering/${promptName}.prompt.md`);
  return this.bedrockService.generate({
    systemPrompt,
    userMessage: JSON.stringify(context, null, 2),
  });
}
```

**Why Parallel Generation:**
- 8 independent documents with no cross-dependencies
- Reduces total generation time from ~40s (sequential) to ~5-8s (parallel)
- Individual failures don't block other documents

**Error Handling:**
- Catch per-document generation errors
- Continue generating remaining documents on failure
- Return partial results with error details for failed documents
- Step 8 UI handles display of partial success

**Relationship to Step 8:**
This service replaces the stub service from Item 24. Step 8 calls this service and handles UI/progress display. Service is responsible only for content generation, not file I/O. `M`

28.3. [x] Steering File Writer & Step 8 Integration â€” Write generated steering files to workspace and integrate with Step 8 UI:

**File Writing:**
- Create `.kiro/steering/` directory if not exists
- Write each generated document from Item 28.2 to corresponding file
- Emit progress events for Step 8 UI updates

**Conflict Handling:**
- Check if `.kiro/steering/` directory exists with files
- If exists, prompt: "Overwrite existing steering files?"
- Options: "Overwrite", "Backup & Overwrite", "Cancel"
- "Backup & Overwrite" copies existing to `.kiro/steering.backup-{timestamp}/`

**Step 8 Integration:**
- Replace stub service calls with real `SteeringGenerationService`
- Update progress UI to show per-file generation status
- Remove `isPlaceholderMode` flag and "Preview mode" indicator
- On success: show generated file list with "Open File" links
- On partial failure: show which files succeeded/failed with retry option

**Post-Generation Actions:**
- Clear wizard state (per Item 22) on full success only
- Keep wizard state on partial failure (allow retry)
- "Open in Kiro" button reveals `.kiro/steering/` folder
- If in Kiro IDE: TODO placeholder for `kiro.startSpecFlow` command (Item 34)

**Validation Before Generation:**
- Verify required wizard steps have content (Steps 1, 3, 5 minimum)
- Show validation errors in Step 8 summary cards
- Block generation if critical steps incomplete

**Files Written:**
```
.kiro/steering/
â”œâ”€â”€ product.md
â”œâ”€â”€ tech.md
â”œâ”€â”€ structure.md
â”œâ”€â”€ customer-context.md
â”œâ”€â”€ integration-landscape.md
â”œâ”€â”€ security-policies.md
â”œâ”€â”€ demo-strategy.md
â””â”€â”€ agentify-integration.md
```

**Demo Script Note:**
`demo-script.md` (Item 23.5) is a separate presenter-focused export to workspace root. `demo-strategy.md` here is Kiro steering for code generation â€” different purpose, different audience. `M`

28.4. [x] Implementation Roadmap Generation (Phase 2) â€” Generate `roadmap.md` from steering files with Kiro usage guidance:

**Trigger:** "Generate Roadmap" button appears in Step 8 UI after Phase 1 steering files are written successfully

**Prompt File:** `resources/prompts/steering/roadmap-steering.prompt.md` (already created)

**Input Context (loaded from generated steering files):**
- `.kiro/steering/tech.md` â€” AgentCore architecture, deployment patterns
- `.kiro/steering/structure.md` â€” Code organization, Strands patterns
- `.kiro/steering/integration-landscape.md` â€” Tools per agent, mock data schemas
- `.kiro/steering/agentify-integration.md` â€” Event contracts, CLI interface

**Roadmap Generation Logic:**
1. Load all 4 input steering files
2. Pass file contents as XML blocks to `roadmap-steering.prompt.md`
3. Generate roadmap via Bedrock
4. Write output to `.kiro/steering/roadmap.md`

**Roadmap Output Structure:**
```markdown
# Implementation Roadmap

## How to Use This Roadmap

This roadmap contains items, each with a prompt for Kiro IDE. Follow this workflow:

1. Open Kiro IDE in this project
2. Copy **Item 1** prompt text (everything in the code block)
3. Paste into Kiro chat â€” Kiro generates spec.md, then requirements, design, tasks, and implements
4. Verify the acceptance criteria on the implemented code
5. Move to **Item 2**, repeat until all items complete

Each item builds on previous ones. Complete them in order.

---

## Architecture Context

> **Include this understanding when reviewing generated code:**
>
> - Agents deploy to **Amazon Bedrock AgentCore Runtime** via AgentCore CLI
> - Only `agents/main.py` runs locally â€” it orchestrates remote agents
> - Use **Strands SDK**: `from strands import Agent, tool`
> - All integrations are **mock tools** (this is a demo system)
> - Emit events to DynamoDB + stdout for Demo Viewer visualization
>
> If generated code runs agents locally or uses real integrations, request corrections.

---

## Item 1: Mock Data Infrastructure
...

## Item 2: {Agent Name} Agent
...
```

**Per-Item Format (enforced by prompt):**
```markdown
## Item N: {Name}

**Purpose:** {one-line description}

**Depends on:** {Item numbers that must be complete first}

**Files to be created:**
- `{path}` â€” {description}

**Prompt for Kiro â€” Copy everything in the code block below and paste into Kiro chat:**

\`\`\`
{Full prompt text with CRITICAL ARCHITECTURE block embedded}
\`\`\`

**Acceptance Criteria (verify after Kiro implements):**
- [ ] {Specific verification step}
- [ ] {Another verification step}
```

**Critical: Architecture Context Embedding**
Every prompt MUST include this block at the top:
```
## CRITICAL ARCHITECTURE â€” READ BEFORE GENERATING CODE

This is an Agentify demo project. Follow these rules strictly:

1. **Agent Deployment**: Agents deploy to Amazon Bedrock AgentCore Runtime
   via `agentcore deploy`. They run REMOTELY, not locally.

2. **Local Orchestrator Only**: Only `agents/main.py` runs locally.
   It orchestrates by calling remote agents.

3. **Strands SDK**: Use `from strands import Agent, tool`

4. **Mock Tools**: All integrations are mocks returning realistic fake data.

5. **Event Emission**: Emit events per .kiro/steering/agentify-integration.md
```

**Item Sequence (generated dynamically from steering files):**
1. **Item 1: Mock Data Infrastructure** â€” Shared mock data files + utilities
2. **Items 2-N: One per agent** â€” Each agent with inline tools, deploys to AgentCore
3. **Final Item: Main Orchestrator** â€” Local `main.py` with CLI contract

**Phase 2 UI (Step 8):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 2: Implementation Roadmap                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  [Generate Roadmap]                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚  Status: [â³] Generating roadmap.md...                      â”‚
â”‚                                                             â”‚
â”‚  Once complete, the roadmap will guide you through:        â”‚
â”‚                                                             â”‚
â”‚  â€¢ Item 1: Mock Data Infrastructure                        â”‚
â”‚  â€¢ Item 2: {Agent A} Agent                                 â”‚
â”‚  â€¢ Item 3: {Agent B} Agent                                 â”‚
â”‚  â€¢ Item N: Main Orchestrator                               â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Post-Generation UI (Success State):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 2: Implementation Roadmap                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  âœ… roadmap.md generated successfully                       â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                     â”‚   â”‚
â”‚  â”‚  ğŸ“‹ How to implement with Kiro:                     â”‚   â”‚
â”‚  â”‚                                                     â”‚   â”‚
â”‚  â”‚  1. Open roadmap.md                                 â”‚   â”‚
â”‚  â”‚  2. Copy the prompt from Item 1                     â”‚   â”‚
â”‚  â”‚  3. Paste into Kiro chat                            â”‚   â”‚
â”‚  â”‚  4. Kiro creates spec.md â†’ requirements â†’ design    â”‚   â”‚
â”‚  â”‚     â†’ tasks â†’ implementation                        â”‚   â”‚
â”‚  â”‚  5. Verify the acceptance criteria                  â”‚   â”‚
â”‚  â”‚  6. Repeat with Item 2, 3, ... in order            â”‚   â”‚
â”‚  â”‚                                                     â”‚   â”‚
â”‚  â”‚  Each item builds on previous ones.                 â”‚   â”‚
â”‚  â”‚  Complete them sequentially.                        â”‚   â”‚
â”‚  â”‚                                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚  [Open roadmap.md]  [Open Folder in Kiro]  [Start Over]    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Error Handling:**
- If any required steering file missing, show error with "Regenerate Steering Files" button
- If roadmap generation fails, show retry button with error details
- Partial generation not supported â€” roadmap is all-or-nothing

**Files Written:**
- `.kiro/steering/roadmap.md`

**Integration with Step 8 Flow:**
- Phase 1 button: "Generate Steering Files" (existing from 28.3)
- Phase 1 complete â†’ Phase 2 section appears
- Phase 2 button: "Generate Roadmap"
- Phase 2 complete â†’ Usage instructions + action buttons appear
- "Start Over" clears wizard state and returns to Step 1 `M`

28.5. [x] CDK Infrastructure Bundling & Extraction â€” Replace CloudFormation SDK deployment with bundled CDK infrastructure:

**File Extraction on "Initialize Project":**
- Extract bundled `resources/cdk/` to `{workspace}/cdk/` (full CDK project with NetworkingStack + ObservabilityStack)
- Extract bundled `resources/scripts/` to `{workspace}/scripts/` (setup.sh, destroy.sh, Dockerfile template)
- Auto-open `cdk/README.md` in editor with deployment instructions

**User-Driven Deployment:**
- User runs `./scripts/setup.sh --region {region}` manually
- setup.sh outputs to `.agentify/infrastructure.json` (deployment info)
- Clear instructions include CDK bootstrap (first-time), deployment command, cost estimate (~$32/mo)

**Config Architecture:**
- Keep separate files: `infrastructure.json` (deployment outputs), `config.json` (extension settings)
- Extension reads from `infrastructure.json` when present for DynamoDB table name/region

**Code Cleanup:**
- Remove `src/services/cloudFormationService.ts`
- Remove `infrastructure/dynamodb-table.yaml`
- Remove `infrastructure/` folder
- Update `initializeProject.ts` to use file extraction instead of SDK calls

**Bundling:**
- Ensure `resources/cdk/` and `resources/scripts/` included in VSIX package
- Add `cdk/README.md` with comprehensive deployment instructions `M`

28.6. [x] Shared Utilities Bundling â€” Bundle generic observability infrastructure as pre-existing resources (like CDK), not Kiro-generated:

**Background:**
The `agents/shared/` module contains generic observability infrastructure that is identical across all Agentify demos:
- `instrumentation.py` â€” `@instrument_tool` decorator, context management (`set_instrumentation_context`, `clear_instrumentation_context`)
- `dynamodb_client.py` â€” Fire-and-forget event persistence to DynamoDB, query for Demo Viewer
- `__init__.py` â€” Module exports with integration examples

This code is 100% generic (no hardcoded agent names, no tool-specific logic, no domain assumptions) and should be bundled like CDK infrastructure rather than regenerated by Kiro for each project.

**New Resources to Create:**
```
resources/agents/shared/
â”œâ”€â”€ __init__.py              # Module exports with integration examples
â”œâ”€â”€ instrumentation.py       # @instrument_tool decorator, context management
â”œâ”€â”€ dynamodb_client.py       # Fire-and-forget event persistence
â””â”€â”€ utils/
    â””â”€â”€ __init__.py          # Placeholder for future utilities
```

Source: Copy validated implementation from test-agentify project after Kiro roadmap validation.

**Extension Changes:**

1. **`src/services/resourceExtractionService.ts`:**
   - Add `agents` folder to extraction alongside `cdk` and `scripts`
   - Ensure Python files get proper permissions

2. **`src/commands/initializeProject.ts`:**
   - Extract `resources/agents` â†’ `workspace/agents` during project initialization
   - Creates `agents/shared/` with pre-bundled utilities

**Steering Prompt Updates (4 files):**

| File | Change |
|------|--------|
| `agentify-integration-steering.prompt.md` | Remove full module implementations (~300 lines for instrumentation.py and dynamodb_client.py). Add "Pre-Bundled Module" section. Show import patterns only: `from agents.shared.instrumentation import instrument_tool, set_instrumentation_context` |
| `structure-steering.prompt.md` | Add explicit note that `agents/shared/` is pre-bundled. Mark as "(pre-bundled - DO NOT MODIFY)" in directory structure. |
| `tech-steering.prompt.md` | Clarify in tool deployment section that shared instrumentation already exists. Add note: "The `agents.shared` module is pre-bundled. Import it, don't recreate it." |
| `roadmap-steering.prompt.md` | Remove "Shared Utilities" as a generated roadmap item since it's pre-bundled. Roadmap should start with Gateway Lambda handlers or first agent implementation. |

**No changes needed:**
- `product-steering.prompt.md` â€” No implementation details
- `customer-context-steering.prompt.md` â€” No implementation details
- `demo-strategy-steering.prompt.md` â€” No implementation details
- `security-policies-steering.prompt.md` â€” No implementation details
- `integration-landscape-steering.prompt.md` â€” References existing patterns

**Documentation Update:**

Update `agent-os/product/demo-development-workflow.md`:
- Add `agents/shared/` to project structure diagram with "(pre-bundled)" annotation
- Update "Key distinction" section: `agents/shared/` = Pre-bundled utilities (DO NOT MODIFY)
- Update summary table: "Shared utilities" created by "Agentify (bundled)" not "Kiro"

**Updated Project Structure (End State):**
```
project/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ shared/                 # Pre-bundled (DO NOT MODIFY)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ instrumentation.py  # @instrument_tool decorator
â”‚   â”‚   â”œâ”€â”€ dynamodb_client.py  # Event persistence
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # Kiro generated - local orchestrator
â”‚   â”œâ”€â”€ analyzer.py             # Kiro generated - agent handlers
â”‚   â””â”€â”€ ...
â”œâ”€â”€ cdk/                        # Pre-bundled (DO NOT MODIFY stacks/)
â””â”€â”€ scripts/                    # Pre-bundled
```

**Benefits:**
1. **Consistency** â€” Every demo gets identical, tested observability infrastructure
2. **No regeneration** â€” Kiro focuses on agent logic, not infrastructure
3. **Demo Viewer compatibility** â€” Guaranteed event schema match with extension
4. **Faster iteration** â€” Improvements to shared utilities benefit all projects

**Validation (Before Implementation):**
- Complete Kiro roadmap in test-agentify project
- Verify shared utilities work correctly with generated agents
- Confirm no project-specific modifications needed
- Validate Demo Viewer correctly consumes events from bundled utilities

**Implementation Steps:**
1. Copy validated `test-agentify/agents/shared/*` â†’ `agentify/resources/agents/shared/`
2. Update `resourceExtractionService.ts` to extract `agents` folder
3. Update `initializeProject.ts` to include agents in extraction
4. Update 4 steering prompt files (see table above)
5. Update `demo-development-workflow.md` documentation
6. Test: Initialize new project, verify `agents/shared/` extracted correctly
7. Test: Run Kiro roadmap on new project, verify agents import from pre-bundled shared `M`

29. [x] Agentify Power Package â€” Create Kiro Power that bundles steering guidance and enforcement hooks. **This is a generic package installed during project initialization (extends Item 4), not per-ideation.** Ensures all agent code follows Agentify patterns from day one:

**Power Structure:**
```
agentify-power/
â”œâ”€â”€ POWER.md              # Steering for agentic workflow development
â””â”€â”€ hooks/
    â”œâ”€â”€ observability-enforcer.kiro.hook
    â”œâ”€â”€ cli-contract-validator.kiro.hook
    â”œâ”€â”€ tool-pattern.kiro.hook
    â””â”€â”€ gateway-handler.kiro.hook
```

**POWER.md Content:**

1. **Pre-Bundled Infrastructure (CRITICAL):**
   - `agents/shared/` is pre-bundled â€” import from it, NEVER recreate
   - `from agents.shared.instrumentation import instrument_tool, set_instrumentation_context, clear_instrumentation_context`
   - `from agents.shared.dynamodb_client import write_tool_event`
   - CDK stacks in `cdk/stacks/` are pre-existing â€” DO NOT MODIFY

2. **Decorator Order (CRITICAL):**
   ```python
   @tool                    # FIRST (inner wrapper - Strands SDK)
   @instrument_tool         # ON TOP (outer wrapper - observability)
   def my_tool():
       ...
   ```
   Wrong order breaks instrumentation silently.

3. **Agent Handler Pattern:**
   ```python
   try:
       set_instrumentation_context(session_id, 'agent_name')
       # invoke agent
   finally:
       clear_instrumentation_context()
   ```

4. **Gateway Lambda Handler Pattern:**
   - Location: `cdk/gateway/handlers/{tool_name}/handler.py`
   - Mock data: `mock_data.json` in same directory (bundled with Lambda)
   - Load via: `os.path.join(os.path.dirname(__file__), 'mock_data.json')`
   - Return: `json.dumps(result)` â€” always return JSON string

5. **Event Emission Patterns:**
   - Reference `.kiro/steering/agentify-integration.md` for event schemas
   - stdout: `graph_structure`, `node_start`, `node_stop`, `workflow_complete`
   - DynamoDB: `tool_call` events via `@instrument_tool` decorator

6. **AgentCore CLI Deployment:**
   - Agents deploy to Bedrock AgentCore Runtime via `agentcore deploy`
   - Only `agents/main.py` runs locally â€” it orchestrates remote agents

7. **Common Pitfalls:**
   - DON'T recreate `agents/shared/` â€” it's pre-bundled
   - DON'T use wrong decorator order â€” `@tool` must be FIRST
   - DON'T forget `clear_instrumentation_context()` in finally block
   - DON'T reference external files in Lambda handlers â€” mock data must be co-located

**Activation Keywords:**
- "agent", "workflow", "Strands", "orchestrator", "demo", "multi-agent", "tool", "handler"

**Distribution:**
- Bundled with Agentify extension in `resources/agentify-power/`
- Can be published to Kiro community powers for standalone use `M`

30. [x] Observability Enforcement Hook â€” Create `observability-enforcer.kiro.hook`:

**Trigger:**
- Event: `fileSaved`
- Pattern: `agents/**/*.py` (includes nested directories like `agents/analyzer/tools/`)

**Validation Rules:**

1. **Pre-Bundled Import Check (CRITICAL):**
   - Verify import: `from agents.shared.instrumentation import instrument_tool`
   - Flag if file contains instrumentation code that duplicates `agents/shared/`
   - Error if file tries to define `@instrument_tool` decorator locally

2. **Decorator Order Check (CRITICAL):**
   - For any function with both `@tool` and `@instrument_tool`:
   - Verify `@tool` appears BEFORE `@instrument_tool` (reading top-to-bottom)
   - Error message: "Wrong decorator order. @tool must be FIRST, @instrument_tool ON TOP"

3. **Handler Context Management:**
   - In `*_handler.py` files, verify pattern:
     - `set_instrumentation_context(session_id, ...)` in try block
     - `clear_instrumentation_context()` in finally block
   - Warning if context not cleared in finally

4. **Event Emission (for main.py only):**
   - Check for `emit_stdout_event()` or equivalent
   - Verify `workflow_id` and `trace_id` in events
   - Check `graph_structure` emitted before `node_start`
   - Verify terminal event (`workflow_complete` or `workflow_error`)

**Hook Prompt:**
```
Review this agent file for Agentify observability compliance.
Reference: .kiro/steering/agentify-integration.md

CRITICAL CHECKS:
1. Import from agents.shared.instrumentation â€” NEVER recreate this module
2. Decorator order: @tool FIRST, then @instrument_tool ON TOP
3. Handler pattern: set_instrumentation_context in try, clear in finally

ADDITIONAL CHECKS:
4. All agent handlers emit node_start/node_stop events (if main.py)
5. workflow_id and trace_id included in all events
6. Terminal event emitted on completion/error

Flag violations with specific line numbers and fix suggestions.
```

**Output:**
- Error for wrong decorator order (blocks save)
- Error for recreating pre-bundled code (blocks save)
- Warning for missing context cleanup
- Inline suggestions for missing event emissions `M`

31. [x] CLI Contract Validation Hook â€” Create `cli-contract-validator.kiro.hook`:

**Trigger:**
- Event: `fileSaved`
- Pattern: `agents/main.py`

**Validation Rules:**
- Check `argparse` setup exists
- Verify `--prompt` argument defined and required
- Verify `--workflow-id` argument defined and required
- Verify `--trace-id` argument defined and required
- Check `os.environ.get('AGENTIFY_TABLE_NAME')` present
- Check `os.environ.get('AGENTIFY_TABLE_REGION')` present

**Hook Prompt:**
Validate this main.py entry point against Agentify CLI contract.
Reference: .kiro/steering/agentify-integration.md Section 1
Required CLI arguments: --prompt, --workflow-id, --trace-id
Required env vars: AGENTIFY_TABLE_NAME, AGENTIFY_TABLE_REGION
Flag any missing arguments or environment variable reads.
Suggest argparse setup if not present.
`````S`

32. [x] Tool Pattern Hook â€” Create `tool-pattern.kiro.hook` (validates tool implementations):

**Trigger:**
- Event: `fileSaved`
- Pattern: `agents/*/tools/*.py` (tools inside agent directories)

**Validation Rules:**

1. **Pre-Bundled Import Check (CRITICAL):**
   - Verify import: `from agents.shared.instrumentation import instrument_tool`
   - Error if `@instrument_tool` decorator defined locally

2. **Decorator Order Check (CRITICAL):**
   - For functions with both decorators:
   - `@tool` must appear FIRST (closer to function)
   - `@instrument_tool` must appear ON TOP (outer wrapper)
   - Example of CORRECT order:
     ```python
     @tool                    # FIRST
     @instrument_tool         # ON TOP
     def my_tool():
     ```

3. **Strands SDK Compliance:**
   - Check for `@tool` decorator from `strands` package
   - Verify function has docstring (used by Strands for tool description)
   - Check return type annotation present

4. **Mock Data Validation:**
   - If `integration-landscape.md` exists, validate mock response matches schema

**Hook Prompt:**
```
Review this tool file for Agentify compliance.
Reference: .kiro/steering/integration-landscape.md for expected mock data shapes

CRITICAL CHECKS:
1. Import @instrument_tool from agents.shared.instrumentation â€” NEVER define locally
2. Decorator order: @tool FIRST (inner), @instrument_tool ON TOP (outer)

STRANDS CHECKS:
3. @tool decorator present on tool functions
4. Docstring describes what the tool does (Strands uses this)
5. Type hints on parameters and return value
6. Mock response matches schema in integration-landscape.md

Flag decorator order violations as errors. Suggest fixes for other issues.
```

**Output:**
- Error for wrong decorator order (with correct example)
- Error for locally-defined instrumentation
- Warning for missing docstring or type hints
- Suggestions for mock data improvements `S`

32.5. [x] Gateway Lambda Handler Hook â€” Create `gateway-handler.kiro.hook` (validates shared Lambda tools):

**Trigger:**
- Event: `fileSaved`
- Pattern: `cdk/gateway/handlers/*/handler.py`

**Validation Rules:**

1. **Tool Name Parsing:**
   - Check for `context.client_context.custom.get('bedrockAgentCoreToolName', '')`
   - Verify delimiter parsing: `tool_name.split('___')[-1]` or equivalent

2. **Mock Data Loading (CRITICAL):**
   - Verify mock data loaded from same directory: `os.path.dirname(__file__)`
   - Error if referencing external paths like `../../mocks/`
   - Check for `mock_data.json` file existence check

3. **Return Format:**
   - Verify return value is `json.dumps(result)` â€” must be JSON string
   - Error if returning dict directly (Gateway expects string)

4. **No External Dependencies:**
   - Warning if importing from paths outside handler directory
   - Lambda deployment packages are isolated

**Hook Prompt:**
```
Review this Gateway Lambda handler for Agentify compliance.
Reference: .kiro/steering/tech.md for Gateway patterns

CRITICAL CHECKS:
1. Tool name parsed from context.client_context.custom['bedrockAgentCoreToolName']
2. Mock data loaded from os.path.dirname(__file__) â€” NEVER external paths
3. Return value is json.dumps(result) â€” Gateway expects JSON string

ISOLATION CHECKS:
4. No imports from outside handler directory
5. mock_data.json exists in same directory

Flag external file references as errors. Suggest fixes for return format issues.
```

**Output:**
- Error for external file references
- Error for non-string return values
- Warning for missing mock_data.json
- Suggestions for proper tool name parsing `S`

33. [x] Power Installation Integration â€” **Extend Project Initialization (Item 4)** to install Agentify Power during "Agentify: Initialize Project" command:

**Installation Flow:**
1. After writing steering files, check if Agentify Power installed
2. If not installed, copy from `resources/agentify-power/` to `.kiro/powers/agentify/`
3. Register power in `.kiro/powers/manifest.json`

**Power Manifest:**
```json
{
  "powers": [
    {
      "name": "agentify",
      "path": "./agentify",
      "activationKeywords": ["agent", "workflow", "Strands", "orchestrator", "demo"]
    }
  ]
}
```

**Verification:**
- Validate hooks are syntactically correct
- Show notification: "Agentify Power installed - enforcement hooks active" `S`

## Phase 3.5: Conversational Workflows

35. [x] Demo Viewer Chat UI â€” Replace single prompt textarea with chat-style conversation (reuse Ideation Wizard Step 2 patterns):

**UI Layout:**
- Message bubbles (user right-aligned, agent left-aligned)
- Streaming response display during agent execution
- Inline agent status: "Triage âœ“ â†’ Technical (pending)" (text-based before graph)
- Session info bar: workflow_id, turn count, elapsed time
- "New Conversation" button to reset session

**Reuse from Ideation Wizard:**
- Streaming token handling from Step 2 (`handleStreamingToken`)
- Message bubble styling from Step 2 conversation UI
- "Send" button pattern (no Enter shortcut)

**Files:**
- `src/panels/demoViewerPanel.ts` â€” Replace prompt section with chat UI
- New CSS in webview for message bubbles `M`

35.1. [x] Dual-Pane Conversation UI â€” Split Demo Viewer chat into side-by-side panes for clearer conversation flow:

**Left Pane: Human â†” Entry Agent**
- User messages (blue, right-aligned) labeled "Human"
- Entry agent responses only (gray, left-aligned)
- Entry agent identified from `get_entry_agent()` / first node_start

**Right Pane: Agent â†” Agent Collaboration**
- Agent handoff prompts (blue, right-aligned) labeled with sending agent name
- Receiving agent responses (gray, left-aligned)
- Shows internal collaboration invisible to end users

**Event Changes:**
- Add `handoff_prompt` to `node_start` event (the prompt sent to agent)
- Add `from_agent` to `node_start` event (null for entry agent)

**Routing Logic:**
- Entry agent responses â†’ Left pane (user-facing)
- All other agent responses â†’ Right pane (internal)
- Agent-to-agent prompts visible in right pane

**Files:**
- `src/types/chatPanel.ts`, `src/utils/chatStateUtils.ts`, `src/panels/demoViewerChatLogic.ts`
- `src/panels/demoViewerChatStyles.ts`, `src/utils/chatPanelHtmlGenerator.ts`
- `resources/agents/main_*.py` templates (add handoff_prompt, from_agent fields) `M`

35.2. [x] Tool Call Visualization â€” Show tool calls inline with agent messages in collaboration pane:

**Data Source:**
- DynamoDB stores tool events via `@instrument_tool` decorator
- Events include: `workflow_id`, `agent`, `tool_name`, `parameters`, `status`, `duration_ms`
- Polling already fetches these events (existing DynamoDB integration)

**UI Design (inline chips below agent messages):**
```
â”Œâ”€ AGENT COLLABORATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Triage Agent                                      â”‚
â”‚ "Analyzing this customer's situation..."          â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ â”‚ ğŸ”§ lookup_customer âœ“ 0.3s  ğŸ”§ get_ticket âœ“   â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                   â”‚
â”‚                       Escalation Handler          â”‚
â”‚                      "Based on VIP status..."     â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                      â”‚ ğŸ”§ notify_manager âœ“     â”‚ â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tool Chip States:**
- â³ Running (animated): Tool started, waiting for completion
- âœ“ Completed (green): Tool finished successfully with duration
- âœ— Failed (red): Tool errored with hover tooltip for error message

**Implementation:**
- Match tool events to agent messages by `agent` field and timestamp
- Group consecutive tool calls under same agent message
- Optionally expandable to show parameters/output (click to expand)

**Files:**
- `src/types/events.ts` â€” Already has `ToolCallEvent` interface
- `src/types/chatPanel.ts` â€” Add `toolCalls: ToolCallEvent[]` to `ChatMessage`
- `src/utils/chatStateUtils.ts` â€” Match tool events to agent messages
- `src/panels/demoViewerChatStyles.ts` â€” Tool chip CSS styles
- `src/utils/chatPanelHtmlGenerator.ts` â€” Render tool chips inline `M`

37. [x] Partial Execution Detection â€” Detect and handle "needs more info" workflow pauses:

**Detection Strategy (simplified by 35.1):**
- Entry agent identified from first `node_start` (per 35.1)
- Partial execution = entry agent's `node_stop` received WITHOUT subsequent `workflow_complete`
- This means: entry agent responded (asking for info) but workflow paused

**Behavior:**
- Partial: Entry agent response in LEFT pane, input enabled, "Awaiting your response..." indicator
- Complete: Final result shown, input enabled for new queries

**UI Indicators:**
- Partial: Subtle "..." or typing indicator in left pane after entry agent bubble
- Complete: Green checkmark or "Complete" badge

**Integration with Dual-Pane (35.1):**
- Partial execution naturally shows in LEFT pane (entry agent asking for info)
- RIGHT pane may show some internal processing before pause
- Follow-up continues the same session (item 38)

**Files:**
- `src/panels/demoViewerChatLogic.ts` â€” Partial execution detection in event handlers
- `src/utils/chatStateUtils.ts` â€” Partial state tracking `S`

38. [x] Workflow Session Continuation â€” Enable multi-turn conversations within same workflow session:

**Session State (builds on 35.1 dual-pane):**
- `WorkflowTriggerService` maintains: `{workflowId, sessionId, conversationTurns[]}`
- Each turn = `{userPrompt, entryAgentResponse, timestamp}`
- Internal agent collaboration (right pane) NOT included in conversation context
- DynamoDB events linked by same workflow_id across turns

**main.py Template Changes:**
- New `--conversation-context` arg: JSON array of human â†” entry agent exchanges only
- Orchestrator builds combined prompt from conversation history
- Entry agent sees full conversation; internal agents see current context only

**Session Lifecycle:**
- Start: User sends first prompt â†’ generate workflow_id/session_id
- Continue: User sends follow-up â†’ same IDs, append to conversationTurns
- Complete: workflow_complete with no pending question OR user clicks "New Conversation"

**Integration with Dual-Pane UI:**
- Follow-up user messages appear in LEFT pane
- New agent collaboration appears in RIGHT pane (fresh per turn)
- Turn indicator shows: "Turn: 2" in session bar

**Files:**
- `src/services/workflowTriggerService.ts` â€” Session state management
- `src/utils/chatStateUtils.ts` â€” Turn tracking utilities
- `resources/agents/shared/orchestrator_utils.py` â€” Parse conversation context arg `L`

39. [ ] Cross-Agent Memory â€” Enable agents to share context via AgentCore Memory, reducing duplicate external calls:

**Problem:**
In multi-agent workflows, earlier agents often fetch data that later agents also need:
- Triage Agent: `get_ticket()` â†’ `get_customer()` â†’ classify â†’ route
- Technical Agent: `get_ticket()` â†’ `get_customer()` â†’ search KB â†’ respond
- Same data fetched twice (latency, cost, inconsistency risk)

**Solution: Memory as a Tool (Option B)**
Provide `search_memory()` and `store_context()` tools that agents can use. Agent prompts include generic guidance to check memory before calling external tools. LLM decides when to apply the pattern based on context.

**Why Option B (vs. auto-injection):**
- **Demo visibility**: Tool calls appear in execution log ("ğŸ”§ search_memory â†’ ticket context")
- **Agent control**: Agent decides when memory lookup is relevant
- **Graceful degradation**: Returns "no context found" if memory empty
- **Simple implementation**: Two tools, one prompt addition

**Wizard Step 4 Updates (Security & Guardrails):**

Add memory configuration to Step 4 UI:

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `crossAgentMemoryEnabled` | Toggle | `true` | Enable memory sharing between agents in same workflow |
| `memoryExpiryDays` | Dropdown | `7` | How long to retain memory (1, 7, 30 days) |

**UI Layout:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Security & Guardrails                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Data Sensitivity: [Confidential â–¼]                          â”‚
â”‚ Compliance: â˜‘ SOC 2  â˜ HIPAA  â˜ PCI-DSS                    â”‚
â”‚                                                             â”‚
â”‚ â”€â”€ Memory Settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â˜‘ Enable cross-agent memory sharing                         â”‚
â”‚   Allows agents to share fetched data within a workflow,    â”‚
â”‚   reducing duplicate API calls and improving consistency.   â”‚
â”‚                                                             â”‚
â”‚   Memory retention: [7 days â–¼]                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Files:**
- `src/panels/ideationStep4Logic.ts` â€” Add memory toggle state
- `src/panels/ideationStep4Html.ts` â€” Add memory UI section
- `src/types/wizardPanel.ts` â€” Add `SecurityGuardrailsState.crossAgentMemoryEnabled`

**Configuration Schema** (`.agentify/config.json`):
```json
{
  "memory": {
    "crossAgent": {
      "enabled": true,
      "expiryDays": 7
    }
  }
}
```

The wizard writes this config, and `setup-memory.sh` reads it to decide whether to create the memory resource.

**Infrastructure Setup (AgentCore CLI):**

Memory is created via AgentCore CLI (not CDK), following the documented AgentCore pattern. A dedicated `setup-memory.sh` script handles creation, following the same pattern as `setup-gateway.sh`.

**New Script** (`resources/scripts/setup-memory.sh`):
```bash
#!/bin/bash
# Creates AgentCore Memory resource for cross-agent context sharing.
# Reads config from .agentify/config.json, writes MEMORY_ID to infrastructure.json.

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
CONFIG_JSON="${PROJECT_ROOT}/.agentify/config.json"
INFRA_JSON="${PROJECT_ROOT}/.agentify/infrastructure.json"

# Load configuration
PROJECT_NAME=$(jq -r '.projectName // "agentify-project"' "$CONFIG_JSON")
REGION=$(jq -r '.aws.region // "us-east-1"' "$CONFIG_JSON")
MEMORY_ENABLED=$(jq -r '.memory.crossAgent.enabled // true' "$CONFIG_JSON")
EXPIRY_DAYS=$(jq -r '.memory.crossAgent.expiryDays // 7' "$CONFIG_JSON")

# Check if memory is enabled in config
if [ "$MEMORY_ENABLED" != "true" ]; then
    echo "Cross-agent memory disabled in config, skipping..."
    exit 0
fi

MEMORY_NAME="${PROJECT_NAME}-cross-agent-memory"

# Check if memory already exists
echo "Checking for existing memory resource..."
EXISTING=$(agentcore memory list --region "$REGION" 2>/dev/null | grep "$MEMORY_NAME" || true)

if [ -n "$EXISTING" ]; then
    MEMORY_ID=$(echo "$EXISTING" | awk '{print $1}')
    echo "Memory already exists: $MEMORY_ID"
else
    # Create new memory resource
    echo "Creating AgentCore Memory: $MEMORY_NAME"
    MEMORY_OUTPUT=$(agentcore memory create "$MEMORY_NAME" \
        --strategies '[{"semanticMemoryStrategy": {"name": "AgentContext"}}]' \
        --event-expiry-days "$EXPIRY_DAYS" \
        --region "$REGION" \
        --wait 2>&1)

    # Extract memory ID from output
    MEMORY_ID=$(echo "$MEMORY_OUTPUT" | grep -oP 'Memory ID: \K[^\s]+' || \
        agentcore memory list --region "$REGION" | grep "$MEMORY_NAME" | awk '{print $1}')

    if [ -z "$MEMORY_ID" ]; then
        echo "Error: Failed to create memory resource"
        echo "Output: $MEMORY_OUTPUT"
        exit 1
    fi

    echo "Memory created: $MEMORY_ID"
fi

# Update infrastructure.json with memory config
echo "Updating infrastructure.json..."
jq --arg mid "$MEMORY_ID" '.memory.memoryId = $mid' \
    "$INFRA_JSON" > tmp.json && mv tmp.json "$INFRA_JSON"

echo "âœ“ Cross-agent memory configured: $MEMORY_ID"
```

**Setup Script Updates** (`scripts/setup.sh`):
```bash
# Step 2b: Create Cross-Agent Memory (after CDK, before agents)
echo ""
echo "Step 2b: Setting up cross-agent memory..."
./scripts/setup-memory.sh

# Load MEMORY_ID for agent deployment
MEMORY_ID=$(jq -r '.memory.memoryId // empty' "${PROJECT_ROOT}/.agentify/infrastructure.json")
```

**Agent Deployment Updates** (`scripts/setup.sh` Step 3):
```bash
# Deploy agents with MEMORY_ID environment variable
echo "Deploying agents to AgentCore Runtime..."

# Load memory ID from infrastructure.json
MEMORY_ID=$(jq -r '.memory.memoryId // empty' "${PROJECT_ROOT}/.agentify/infrastructure.json")

# Pass MEMORY_ID as env var to agent (AgentCore passes to runtime)
DEPLOY_ENV_ARGS="--env AGENTIFY_PROJECT_NAME=${PROJECT_NAME}"
if [ -n "$MEMORY_ID" ]; then
  DEPLOY_ENV_ARGS="${DEPLOY_ENV_ARGS} --env MEMORY_ID=${MEMORY_ID}"
fi

uv run agentcore deploy -a "${AGENT_NAME}" --auto-update-on-conflict ${DEPLOY_ENV_ARGS}
```

**Orchestrator Script Updates** (`scripts/orchestrate.sh`):
```bash
# Load memory ID from infrastructure.json for local orchestrator
MEMORY_ID=""
if [ -f "${INFRA_JSON}" ] && command -v jq &> /dev/null; then
  MEMORY_ID=$(jq -r '.memory.memoryId // empty' "${INFRA_JSON}" 2>/dev/null)
fi

# Export for Python orchestrator
if [ -n "$MEMORY_ID" ]; then
  export MEMORY_ID="${MEMORY_ID}"
fi

# Run main.py - it reads MEMORY_ID from env and initializes memory client
uv run python agents/main.py \
    --prompt "$PROMPT" \
    --workflow-id "$WORKFLOW_ID" \
    --trace-id "$TRACE_ID" \
    --turn-number 1 \
    > "$JSON_OUTPUT" 2> >(tee "$STDERR_FILE" >&2)
```

**Destroy Script Updates** (`scripts/destroy.sh`):
```bash
# Delete memory resource during cleanup (before CDK destroy)
echo "Deleting AgentCore Memory..."
MEMORY_ID=$(jq -r '.memory.memoryId // empty' .agentify/infrastructure.json 2>/dev/null)
if [ -n "$MEMORY_ID" ]; then
  agentcore memory delete $MEMORY_ID --region $REGION --wait || true
  echo "Memory resource deleted: $MEMORY_ID"
fi
```

**Environment Variable Propagation:**
- **Remote agents**: `MEMORY_ID` passed via `agentcore deploy --env` â†’ agent reads via `os.getenv('MEMORY_ID')`
- **Local orchestrator**: `orchestrate.sh` exports `MEMORY_ID` â†’ `main.py` reads and calls `init_memory()`
- **Demo Viewer**: `WorkflowTriggerService` reads from `infrastructure.json` and passes as env var to subprocess

**New Pre-Bundled Module:**
```
resources/agents/shared/
â”œâ”€â”€ memory_client.py    # search_memory, store_context tools
â””â”€â”€ ...existing files...
```

**memory_client.py Implementation:**
```python
import os
from strands import tool
from agents.shared.instrumentation import instrument_tool

_memory_client = None
_memory_id = None
_session_id = None

def init_memory(session_id: str):
    """Initialize memory for session. Called by orchestrator."""
    global _memory_client, _memory_id, _session_id
    _memory_id = os.environ.get('MEMORY_ID')
    if not _memory_id:
        print("Warning: MEMORY_ID not set, cross-agent memory disabled")
        return
    _session_id = session_id
    from bedrock_agentcore.memory import MemoryClient
    _memory_client = MemoryClient(region_name=os.environ.get('AWS_REGION', 'us-west-2'))

@tool
@instrument_tool
def search_memory(query: str) -> str:
    """Search shared memory for context from previous agents in this workflow.

    Use BEFORE calling external tools to check if data was already fetched.
    Memory is scoped to the current session - other users/workflows cannot access.
    """
    if not _memory_client or not _memory_id or not _session_id:
        return "Memory not initialized. Use external tools."
    try:
        # retrieve_memories() is the correct SDK method for semantic search
        # Searches LTM records extracted by memory strategies from stored events
        results = _memory_client.retrieve_memories(
            memory_id=_memory_id,
            namespace=f"/workflow/{_session_id}/context",
            query=query,
            top_k=3
        )
        if results:
            return "\n".join([r.get('content', '') for r in results])
        return "No relevant context found."
    except Exception as e:
        print(f"Memory search error: {e}")
        return "Memory search failed. Use external tools."

@tool
@instrument_tool
def store_context(key: str, value: str) -> str:
    """Store findings for downstream agents in this workflow.

    Memory is scoped to the current session - other users/workflows cannot access.
    Note: Data stored via create_event() is auto-extracted to LTM by memory strategies.
    """
    if not _memory_client or not _memory_id or not _session_id:
        return "Memory not initialized."
    try:
        # create_event() stores as conversational event
        # Semantic strategy auto-extracts to LTM for retrieve_memories() searches
        _memory_client.create_event(
            memory_id=_memory_id,
            actor_id="agent",
            session_id=_session_id,
            messages=[(f"[{key}]: {value}", "assistant")]
        )
        return f"Stored: {key}"
    except Exception as e:
        print(f"Memory store error: {e}")
        return f"Failed to store: {key}"
```

**Session Isolation:**
- `session_id` = workflow session ID (from `--workflow-id` or generated)
- Each workflow run gets a unique session ID
- Memory reads/writes are scoped to that session only
- Different users running concurrent workflows cannot access each other's memory
- Same workflow's agents CAN share memory (that's the point)

**Orchestrator Integration (main.py templates):**
```python
# In orchestrator setup
from agents.shared.memory_client import init_memory

# Initialize memory with session_id (same as workflow session)
init_memory(session_id)
```

**Agentify Power Updates** (`resources/agentify-power/POWER.md`):

Add **Pattern 8: Cross-Agent Memory** to POWER.md:
```markdown
## Pattern 8: Cross-Agent Memory

Use `search_memory()` before external calls, `store_context()` after retrieving data.

```python
# CORRECT - Import from shared, use memory tools
from agents.shared.memory_client import init_memory, search_memory, store_context

# In main.py orchestrator - initialize memory first
init_memory(session_id)

# In agent tools - check memory before external calls
@tool
@instrument_tool
def get_customer_info(customer_id: str) -> dict:
    """Get customer information."""
    # Check memory first
    cached = search_memory(f"customer {customer_id}")
    if "No relevant context" not in cached:
        return json.loads(cached)

    # Fetch from external source
    result = external_api.get_customer(customer_id)

    # Store for downstream agents
    store_context(f"customer_{customer_id}", json.dumps(result))
    return result

# WRONG - Never recreate memory_client locally
def search_memory(query):  # BLOCKING ERROR
    ...
```

Environment: `MEMORY_ID` must be set (from infrastructure.json or agentcore deploy).
```

Add to **Quick Checklist**:
```markdown
- [ ] Memory tools imported from `agents.shared.memory_client` (not local)
- [ ] `init_memory(session_id)` called in main.py before agent invocations
- [ ] `search_memory()` used before expensive external API calls
```

**Hook Updates** (`resources/agentify-hooks/`):

1. **observability-enforcer.kiro.hook** â€” Add memory functions to local definition check:
   ```json
   // Add to LOCAL DEFINITIONS list in prompt:
   "- init_memory\n- search_memory\n- store_context"

   // Add to CORRECT imports section:
   "from agents.shared.memory_client import init_memory, search_memory, store_context"
   ```

2. **cli-contract-validator.kiro.hook** â€” Add memory initialization check:
   ```json
   // Add to VERIFY IMPORTS section:
   "- init_memory (from agents.shared.memory_client)"

   // Add to validation:
   "## Memory Initialization (if MEMORY_ID used):\n- init_memory(session_id) should be called in main() before agent invocations\n- Import from agents.shared.memory_client, not defined locally"
   ```

3. **tool-pattern.kiro.hook** â€” No changes needed (already validates decorator order for all `@tool`/`@instrument_tool` functions, which includes memory tools)

**Steering Prompt Updates:**

1. **agentify-integration-steering.prompt.md:**
   - Add `memory_client.py` to Pre-Bundled Shared Utilities table
   - Add new section:
   ```markdown
   ## Cross-Agent Memory

   Agents in a workflow often need data that earlier agents have already retrieved.
   To avoid duplicate external calls and share context:

   - Use `search_memory(query)` before calling external tools to check if relevant data exists
   - Use `store_context(key, value)` after retrieving important data for downstream agents

   This reduces latency, cost, and ensures consistency across the workflow.
   ```

2. **structure-steering.prompt.md:**
   - Add `memory_client.py` to `agents/shared/` directory listing

**Agent Prompt Pattern (generic, not tool-specific):**
Generated agent prompts will include:
```
## Memory Usage
Before calling external tools, use search_memory() to check if previous agents
already retrieved the needed data. Store important findings with store_context()
for downstream agents.
```

The LLM applies this pattern to whatever tools exist in the specific project.

**Demo Viewer Visibility:**
Memory tool calls appear in execution log like any other tool:
```
14:32:02  ğŸ”§ search_memory â†’ "ticket TKT-001 customer"     â† Visible!
14:32:02     â””â”€ Result: "Customer: Acme Corp, enterprise tier..."
14:32:03  ğŸ”§ search_knowledge_base â†’ "API authentication"
```

Sales presenter can point out: "See? It remembered from the previous agent."

**Dependencies:**
- Item 38 (Session continuation) â€” Uses same session_id
- Item 28.6 (Shared utilities bundling) â€” Extends pre-bundled pattern
- AgentCore Memory SDK (`bedrock_agentcore.memory`)

**Files:**

*Wizard UI:*
- `src/panels/ideationStep4Logic.ts` â€” Add memory toggle state handling
- `src/panels/ideationStep4Html.ts` â€” Add memory settings UI section
- `src/types/wizardPanel.ts` â€” Add `SecurityGuardrailsState.crossAgentMemoryEnabled`, `memoryExpiryDays`

*Scripts:*
- `resources/scripts/setup-memory.sh` â€” **New script** for AgentCore Memory creation
- `resources/scripts/setup.sh` â€” Call `setup-memory.sh` in Step 2b, pass `MEMORY_ID` to agent deploy
- `resources/scripts/orchestrate.sh` â€” Load `MEMORY_ID` from infrastructure.json, export for main.py
- `resources/scripts/destroy.sh` â€” Add `agentcore memory delete` for cleanup

*Bundled Code:*
- `resources/agents/shared/memory_client.py` â€” **New pre-bundled module** with memory tools
- `resources/agents/shared/__init__.py` â€” Export memory functions
- `resources/agents/main_graph.py`, `main_swarm.py`, `main_workflow.py` â€” Add `init_memory()` call in DO NOT MODIFY section

*Extension Services:*
- `src/services/workflowTriggerService.ts` â€” Pass `MEMORY_ID` env var (read from infrastructure.json)

*Kiro Guidance:*
- `resources/agentify-power/POWER.md` â€” Add Pattern 8 (memory) and Quick Checklist items
- `resources/agentify-hooks/observability-enforcer.kiro.hook` â€” Add memory functions to local definition check
- `resources/agentify-hooks/cli-contract-validator.kiro.hook` â€” Add `init_memory()` validation
- `resources/prompts/steering/agentify-integration-steering.prompt.md` â€” Add memory section
- `resources/prompts/steering/structure-steering.prompt.md` â€” Add to directory listing

39.5. [ ] Persistent Session Memory â€” Enable agents to learn from past workflow sessions using AgentCore Memory's semantic and event strategies:

**Problem:**
Generated demos often need to remember user preferences, historical interactions, and learned patterns across multiple workflow sessions. Item #39 covers within-session memory sharing, but doesn't address persistent learning that survives across separate workflow runs.

**Use Cases:**
1. **User Preference Learning**: Remember what users like/dislike across sessions (e.g., meal preferences, communication styles)
2. **Historical Context**: Access past interactions when relevant (e.g., "last time you asked about...")
3. **Progressive Personalization**: Improve responses over time based on accumulated feedback
4. **Session Continuity**: Resume interrupted workflows with full context

**AgentCore Memory Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AgentCore Memory                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Short-Term Memory (STM)              Long-Term Memory (LTM)    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚  â€¢ Event-based auto-expiry            â€¢ Semantic strategies     â”‚
â”‚  â€¢ Active session context             â€¢ Vector similarity searchâ”‚
â”‚  â€¢ Workflow state tracking            â€¢ Namespace organization  â”‚
â”‚  â€¢ 7-day default TTL                  â€¢ Permanent until deleted â”‚
â”‚                                                                  â”‚
â”‚  Use: Current workflow session        Use: Cross-session learningâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Memory Strategies (from wizard Step 4 or defaults):**
- `SemanticStrategy`: Natural language queries against past interactions
- `SummaryStrategy`: Condensed summaries of long conversations
- `UserPreferenceStrategy`: Explicit preference storage (likes, dislikes, settings)
- `CustomStrategy`: Domain-specific memory patterns

**Namespace Patterns (generated from wizard context):**
```
/{project}/preferences/{user_id}     # User-specific preferences
/{project}/history/{session_type}    # Historical interactions by type
/{project}/feedback/{entity_type}    # User feedback/ratings
/{project}/context/{domain}          # Domain-specific learned context
```

**Implementation Components:**

1. **Wizard Step 4 Extension (Security & Guardrails):**
   - Add "Memory Persistence" section with:
     - Memory strategy dropdown (Semantic, Summary, User Preference, Custom)
     - Retention policy selector (7 days, 30 days, 1 year, permanent)
     - Namespace prefix configuration
   - AI suggests appropriate strategies based on business objective

2. **New Pre-Bundled Module** (`resources/agents/shared/persistent_memory.py`):
   ```python
   import os
   from typing import Optional
   from strands import tool
   from agents.shared.instrumentation import instrument_tool
   from bedrock_agentcore.memory.session import MemorySessionManager

   _session_manager = None
   _memory_session = None
   _user_id: Optional[str] = None

   def init_persistent_memory(user_id: str, memory_id: Optional[str] = None) -> bool:
       """Initialize persistent memory for cross-session learning."""
       global _session_manager, _memory_session, _user_id
       memory_id = memory_id or os.environ.get('PERSISTENT_MEMORY_ID')
       if not memory_id:
           print("Warning: PERSISTENT_MEMORY_ID not set")
           return False
       _user_id = user_id
       _session_manager = MemorySessionManager(
           memory_id=memory_id,
           region_name=os.environ.get('AWS_REGION', 'us-east-1')
       )
       # Create session for user - enables cross-session persistence
       _memory_session = _session_manager.create_memory_session(
           actor_id=_user_id,
           session_id=f"persistent_{_user_id}"
       )
       return True

   @tool
   @instrument_tool
   def remember_preference(category: str, preference: str, value: str) -> str:
       """Store a user preference for future sessions.

       Note: LTM extraction is AUTOMATIC - you cannot write directly to LTM.
       Preferences stored as turns; user_preference strategy extracts them.
       """
       if not _memory_session:
           return "Persistent memory not initialized."
       try:
           from bedrock_agentcore.memory.constants import ConversationalMessage, MessageRole
           # add_turns() triggers automatic LTM extraction via memory strategies
           content = f"User preference - Category: {category}, {preference}: {value}"
           _memory_session.add_turns(
               messages=[ConversationalMessage(content, MessageRole.ASSISTANT)]
           )
           return f"Remembered: {preference}"
       except Exception as e:
           return f"Failed to remember: {preference}"

   @tool
   @instrument_tool
   def recall_preferences(query: str, category: str = None) -> str:
       """Search past preferences using natural language."""
       if not _memory_session:
           return "Persistent memory not initialized."
       namespace_prefix = f"/preferences/{_user_id}"
       if category:
           namespace_prefix += f"/{category}"

       memories = _memory_session.search_long_term_memories(
           query=query,
           namespace_prefix=namespace_prefix,
           top_k=5
       )
       if memories:
           return "\n".join([m.get('content', '') for m in memories])
       return "No relevant preferences found."

   @tool
   @instrument_tool
   def log_feedback(entity_type: str, entity_id: str, rating: int, notes: str = "") -> str:
       """Log user feedback for learning."""
       if not _memory_session:
           return "Persistent memory not initialized."
       try:
           from bedrock_agentcore.memory.constants import ConversationalMessage, MessageRole
           content = f"User feedback - {entity_type}/{entity_id}: {rating}/5"
           if notes:
               content += f", {notes}"
           # add_turns() triggers automatic LTM extraction
           _memory_session.add_turns(
               messages=[ConversationalMessage(content, MessageRole.ASSISTANT)]
           )
           return f"Feedback logged for {entity_id}"
       except Exception as e:
           return f"Failed to log feedback"
   ```

   **Critical: LTM Extraction is Automatic**
   - You CANNOT write directly to Long-Term Memory
   - `add_turns()` stores conversational events
   - Memory strategies (semantic, user_preference, summary) automatically extract knowledge
   - `search_long_term_memories()` searches the extracted LTM records
   - This is a fundamental AgentCore Memory architecture decision

3. **Steering Prompt Updates:**
   - `agentify-integration-steering.prompt.md`: Add persistent memory patterns
   - `structure-steering.prompt.md`: Include `persistent_memory.py` in shared utilities
   - `tech-steering.prompt.md`: Document memory strategy selection guidance

4. **Setup Script Updates** (`scripts/setup.sh`):
   ```bash
   # Step 2c: Create Persistent Memory (if enabled in config)
   MEMORY_PERSISTENCE=$(jq -r '.memory.persistence.enabled // false' "$CONFIG_JSON")
   if [ "$MEMORY_PERSISTENCE" = "true" ]; then
       STRATEGY=$(jq -r '.memory.persistence.strategy // "semantic"' "$CONFIG_JSON")
       RETENTION=$(jq -r '.memory.persistence.retentionDays // 30' "$CONFIG_JSON")

       PERSISTENT_MEMORY_OUTPUT=$(agentcore memory create "${PROJECT_NAME}-persistent" \
           --strategies "[{\"${STRATEGY}MemoryStrategy\": {\"name\": \"PersistentLearning\"}}]" \
           --event-expiry-days "$RETENTION" \
           --region "${REGION}" \
           --wait 2>&1)

       PERSISTENT_MEMORY_ID=$(echo "$PERSISTENT_MEMORY_OUTPUT" | grep -oP 'Memory ID: \K[^\s]+')

       jq --arg pmid "$PERSISTENT_MEMORY_ID" \
           '.memory.persistentMemoryId = $pmid' \
           "$INFRA_CONFIG" > tmp.json && mv tmp.json "$INFRA_CONFIG"
   fi
   ```

5. **Configuration Schema** (`.agentify/config.json`):
   ```json
   {
     "memory": {
       "crossAgent": {
         "enabled": true,
         "memoryId": "..."
       },
       "persistence": {
         "enabled": true,
         "strategy": "semantic",
         "retentionDays": 30,
         "namespacePrefix": "/myproject",
         "persistentMemoryId": "..."
       }
     }
   }
   ```

6. **Demo Viewer Integration:**
   - Memory operations appear in execution log:
     ```
     14:32:02  ğŸ’¾ remember_preference â†’ /preferences/user123/food
     14:32:03  ğŸ” recall_preferences â†’ "Found 3 relevant preferences"
     14:32:04  â­ log_feedback â†’ dish_rating: 4/5
     ```
   - "Memory Explorer" panel (optional) showing stored preferences

**Relationship to Item #39:**
- Item #39 = `agents/shared/memory_client.py` (cross-agent within workflow)
- This item = `agents/shared/persistent_memory.py` (cross-session learning)
- Both use AgentCore Memory SDK but serve different purposes
- Can be used together: cross-agent for workflow efficiency, persistent for learning

**Wizard Flow:**
1. Step 4 captures memory persistence settings
2. Step 5 (Agent Design) auto-adds memory tools to relevant agents
3. Step 8 generates config and steering with memory patterns
4. Roadmap includes memory initialization in appropriate items

**Files:**
- `resources/agents/shared/persistent_memory.py` â€” New pre-bundled module
- `resources/agents/shared/__init__.py` â€” Export persistent memory functions
- `resources/scripts/setup.sh` â€” Add Step 2c for persistent memory creation
- `resources/scripts/destroy.sh` â€” Add persistent memory cleanup
- `src/panels/tabbedPanel.ts` â€” Add memory persistence UI to Step 4
- `src/types/wizard.ts` â€” Add `MemoryPersistenceConfig` to `SecurityGuardrailsState`
- `resources/prompts/steering/agentify-integration-steering.prompt.md` â€” Add persistent memory section
- `resources/prompts/steering/structure-steering.prompt.md` â€” Include in directory listing
- `resources/agentify-power/POWER.md` â€” Add Pattern 10: Persistent Memory `L`

40. [x] Lightweight Router Model (Haiku) â€” Add optional Haiku-based routing for Graph and Swarm patterns:

**Problem:**
Current routing approaches have trade-offs:
- **Hardcoded routes**: Fast but brittle, can't handle semantic nuance
- **Agent-decided routes**: Flexible but uses full Sonnet model for simple routing decisions (slow, expensive)
- **Classification mapping**: Requires structured output from agents, adds complexity to agent prompts

**Solution: Dedicated Haiku Router**
A lightweight routing agent that uses Claude Haiku (~10x cheaper, ~3x faster than Sonnet) specifically for routing decisions:

```python
# New utility in orchestrator_utils.py
async def route_with_haiku(
    current_agent: str,
    agent_response: str,
    available_agents: List[str],
    routing_context: str  # From steering files
) -> Optional[str]:
    """Use Haiku to determine next agent based on response content."""
    router_prompt = f"""You are a routing classifier. Given this agent response, 
    decide which agent should handle next. Options: {available_agents}
    
    Routing guidance:
    {routing_context}
    
    Agent response:
    {agent_response}
    
    Respond with ONLY the agent name, or 'COMPLETE' if workflow is done."""
    
    result = await invoke_haiku(router_prompt)
    return None if result == 'COMPLETE' else result
```

**Graph Pattern Updates (`main_graph.py`):**
```python
def route_to_next_agent(current_agent: str, response: Dict[str, Any]) -> Optional[str]:
    # Strategy 0: Haiku router (NEW - if enabled)
    if USE_HAIKU_ROUTER:
        return await route_with_haiku(
            current_agent,
            response.get('response', ''),
            get_available_agents(),
            get_routing_context()  # From steering files
        )
    
    # Existing strategies as fallback...
```

**Swarm Pattern Updates (`main_swarm.py`):**
```python
def extract_handoff_from_response(response: Dict[str, Any]) -> Optional[str]:
    # Strategy 0: Haiku router (NEW - if enabled)
    if USE_HAIKU_ROUTER:
        return await route_with_haiku(
            current_agent,
            response.get('response', ''),
            get_available_agents(),
            get_routing_context()
        )
    
    # Existing handoff extraction as fallback...
```

**Configuration (`.agentify/config.json`):**
```json
{
  "routing": {
    "useHaikuRouter": true,
    "routerModel": "global.anthropic.claude-haiku-4-5-20251001-v1:0",
    "fallbackToAgentDecision": true
  }
}
```

**Steering Prompt Updates:**
- `tech-steering.prompt.md`: Add routing configuration section
- `agentify-integration-steering.prompt.md`: Add `get_routing_context()` pattern

**Demo Viewer Visibility:**
Router decisions appear in execution log:
```
14:32:02  ğŸ§­ Haiku Router: "technical_agent" (12ms)
14:32:02  â–¶ Technical Agent activated
```

**When to Use Haiku Router:**
- Complex multi-way routing (5+ possible destinations)
- Semantic routing based on response content/sentiment
- When routing logic shouldn't pollute agent prompts
- High-volume demos where Sonnet routing cost adds up

**When NOT to Use:**
- Simple 2-3 way routing (use classification mapping)
- Static pipelines (use Workflow pattern instead)
- When agent must decide its own handoff (Swarm philosophy)

**Files:**
- `resources/agents/shared/orchestrator_utils.py` â€” Add `route_with_haiku()`, `invoke_haiku()`, `get_routing_context()`
- `resources/agents/main_graph.py` â€” Add Haiku routing as Strategy 0
- `resources/agents/main_swarm.py` â€” Add optional Haiku routing override
- `src/services/configService.ts` â€” Add routing configuration schema
- `resources/prompts/steering/tech-steering.prompt.md` â€” Add routing config section
- `resources/prompts/steering/agentify-integration-steering.prompt.md` â€” Add routing context pattern
- `resources/agentify-power/POWER.md` â€” Add Pattern 9: Haiku Routing `M`

41. [x] Cedar Policy Generation â€” Translate Step 4 security inputs into enforceable AgentCore Policy Engine Cedar policies:

**Problem:**
Step 4 captures security intent (data sensitivity, compliance frameworks, approval gates) but generates only documentation (`security-policies.md`). This Markdown is steering for Kiro, not enforceable runtime policies. AgentCore Policy Engine uses Cedar language for actual enforcement.

**Solution: End-to-End Policy Pipeline**
```
Step 4 Inputs â†’ AI generates Cedar policies â†’ CLI creates Policy Engine â†’ Associate with Gateway
```

**New Prompt File:**
`resources/prompts/steering/cedar-policies.prompt.md` â€” Transforms wizard state into Cedar policy syntax:

```markdown
# Cedar Policy Generation Prompt

Generate Cedar policies for AgentCore Policy Engine based on security requirements.

## Input Context
- Data sensitivity: {securityGuardrails.dataSensitivity}
- Compliance frameworks: {securityGuardrails.complianceFrameworks}
- Approval gates: {securityGuardrails.approvalGates}
- Agents and tools: {agentDesign.confirmedAgents with tools}

## Cedar Syntax Requirements
- Action format: `AgentCore::Action::"TargetName___tool_name"` (triple underscore)
- Resource format: `AgentCore::Gateway::"{{GATEWAY_ARN}}"` (placeholder replaced at deploy)
- Forbid-wins semantics (any forbid overrides permits)
- Use `.contains()` for multi-value checks
- Numeric comparisons: integers use `<`, `>`, `==`; decimals use `.lessThan()`, `.greaterThan()`

## Output Structure
Generate separate .cedar files for each concern:
- `data-access.cedar` â€” Data sensitivity controls
- `approval-gates.cedar` â€” Human approval requirements  
- `compliance/{framework}.cedar` â€” Framework-specific policies (HIPAA, PCI-DSS, etc.)
```

**Generated File Structure:**
```
policies/
â”œâ”€â”€ main.cedar              # Base permit/deny rules
â”œâ”€â”€ data-access.cedar       # Sensitivity-based access controls
â”œâ”€â”€ approval-gates.cedar    # Human approval requirements
â””â”€â”€ compliance/
    â”œâ”€â”€ hipaa.cedar         # If HIPAA selected
    â”œâ”€â”€ pci-dss.cedar       # If PCI-DSS selected
    â””â”€â”€ ...                 # Other frameworks
```

**Example Cedar Output (`approval-gates.cedar`):**
```cedar
// Generated from Agentify Step 4: "Before financial transactions" approval gate
// Gateway ARN placeholder replaced at deploy time
permit(
  principal,
  action == AgentCore::Action::"*___execute_transaction",
  resource == AgentCore::Gateway::"{{GATEWAY_ARN}}"
)
when {
  context.input.amount < 10000
};

forbid(
  principal,
  action == AgentCore::Action::"*___execute_transaction",
  resource == AgentCore::Gateway::"{{GATEWAY_ARN}}"
)
when {
  context.input.amount >= 10000
}
unless {
  context.claims.approval_authority == "finance_committee"
};
```

**setup.sh Integration (Step 2b â€” after Gateway, before Agent):**
```bash
# Step 2b: Create Policy Engine (if Cedar policies exist)
POLICIES_DIR="${PROJECT_ROOT}/policies"
if [ -d "$POLICIES_DIR" ] && [ "$(find "$POLICIES_DIR" -name "*.cedar" 2>/dev/null | head -1)" ]; then
    
    GATEWAY_ARN=$(jq -r '.gateway_arn // empty' "$GATEWAY_CONFIG")
    
    if [ -n "$GATEWAY_ARN" ]; then
        # Create Policy Engine
        POLICY_ENGINE_OUTPUT=$(uv run agentcore policy create-policy-engine \
            --name "${PROJECT_NAME}-policy-engine" \
            --description "Generated by Agentify from Step 4 security inputs" \
            --region "${REGION}" 2>&1)
        
        POLICY_ENGINE_ID=$(echo "$POLICY_ENGINE_OUTPUT" | grep -oE "${PROJECT_NAME}-policy-engine-[a-zA-Z0-9]+" | head -1)
        
        if [ -n "$POLICY_ENGINE_ID" ]; then
            # Create policies from each .cedar file
            for CEDAR_FILE in "$POLICIES_DIR"/*.cedar "$POLICIES_DIR"/**/*.cedar; do
                if [ -f "$CEDAR_FILE" ]; then
                    POLICY_NAME=$(basename "$CEDAR_FILE" .cedar | sed 's/-/_/g')
                    CEDAR_CONTENT=$(cat "$CEDAR_FILE" | sed "s|{{GATEWAY_ARN}}|$GATEWAY_ARN|g")
                    CEDAR_ESCAPED=$(echo "$CEDAR_CONTENT" | jq -Rs .)
                    
                    uv run agentcore policy create-policy \
                        --policy-engine-id "$POLICY_ENGINE_ID" \
                        --name "$POLICY_NAME" \
                        --definition "{\"cedar\":{\"statement\":$CEDAR_ESCAPED}}" \
                        --region "${REGION}"
                fi
            done
            
            # Associate with Gateway (default LOG_ONLY for safety)
            POLICY_MODE=$(jq -r '.policy.mode // "LOG_ONLY"' "$CONFIG_JSON" 2>/dev/null)
            POLICY_ENGINE_ARN="arn:aws:bedrock-agentcore:${REGION}:${ACCOUNT_ID}:policy-engine/${POLICY_ENGINE_ID}"
            
            uv run agentcore gateway update-gateway \
                --arn "$GATEWAY_ARN" \
                --policy-engine-arn "$POLICY_ENGINE_ARN" \
                --policy-engine-mode "$POLICY_MODE" \
                --region "${REGION}"
            
            # Save to infrastructure.json
            jq --arg peid "$POLICY_ENGINE_ID" --arg mode "$POLICY_MODE" \
                '.policy = {"policyEngineId": $peid, "mode": $mode}' \
                "$INFRA_CONFIG" > tmp.json && mv tmp.json "$INFRA_CONFIG"
        fi
    fi
fi
```

**destroy.sh Integration (Step 1b â€” before Gateway cleanup):**
```bash
# Step 1b: Delete Policy Engine
POLICY_ENGINE_ID=$(jq -r '.policy.policyEngineId // empty' "$INFRA_CONFIG" 2>/dev/null)
if [ -n "$POLICY_ENGINE_ID" ]; then
    # Delete all policies first
    POLICY_LIST=$(uv run agentcore policy list-policies \
        --policy-engine-id "$POLICY_ENGINE_ID" --region "${REGION}" 2>&1 || echo "")
    # ... delete each policy ...
    
    # Delete Policy Engine
    uv run agentcore policy delete-policy-engine \
        --policy-engine-id "$POLICY_ENGINE_ID" \
        --region "${REGION}" || true
fi
```

**Configuration Schema Update (`config.json`):**
```json
{
  "policy": {
    "enabled": true,
    "mode": "ENFORCE",  // or "LOG_ONLY" (default)
    "policyEngineId": "pe-abc123..."  // Set by setup.sh
  }
}
```

**Steering Generation Integration:**
- Add `generateCedarPolicies()` to `SteeringGenerationService`
- Call after steering file generation in Step 8
- Write to `policies/` directory (separate from `.kiro/steering/`)

**Demo Viewer Integration:**
Policy events appear in execution log:
```
14:32:02  ğŸ›¡ï¸ Policy: ALLOW get_customer (data-access.cedar)
14:32:03  ğŸ›¡ï¸ Policy: DENY execute_transaction - amount exceeds limit
14:32:03  â¸ï¸  Awaiting approval: Financial transaction $15,000
```

**Files:**
- `resources/prompts/steering/cedar-policies.prompt.md` â€” NEW: Cedar generation prompt
- `src/services/steeringGenerationService.ts` â€” Add `generateCedarPolicies()` method
- `src/types/config.ts` â€” Add policy configuration schema
- `resources/scripts/setup.sh` â€” Add Step 2b (Policy Engine creation) âœ… DONE
- `resources/scripts/destroy.sh` â€” Add Step 1b (Policy Engine deletion) âœ… DONE
- `resources/prompts/steering/tech-steering.prompt.md` â€” Reference Cedar policies
- `resources/prompts/steering/agentify-integration-steering.prompt.md` â€” Add policy event patterns `L`

## Phase 4: Visual Polish

25. [ ] Agent Graph Visualization â€” Add React Flow visualization to Demo Viewer with custom node components showing agent status (pending/running/completed/failed), animated edges during data flow, auto-layout via dagre/elkjs, and pattern-specific layouts:

**Node Components:**
- Custom React Flow node for each agent
- Status indicator: gray (pending), blue pulse (running), green (completed), red (failed)
- Agent name and role displayed
- Tool call count badge

**Edge Styling:**
- Animated dashes during active data flow
- Edge labels for Graph pattern conditions
- Bidirectional arrows for Swarm handoffs

**Layout Algorithms:**
- **Graph**: Dagre top-to-bottom DAG layout with conditional edge routing
- **Swarm**: Force-directed peer-to-peer layout (circular for small graphs)
- **Workflow**: ELKjs layered layout with parallel execution lanes

**Initialization:**
- Read `graph_structure` event from stdout to build initial topology
- Fall back to agent design from `.agentify/config.json` if no event received

**Interaction:**
- Click node to highlight in Execution Log
- Zoom/pan controls
- "Fit to view" button `L`

26. [ ] Graph Animation â€” Implement real-time graph updates from stdout events with smooth transitions:

**Event Handling:**
- `node_start` â†’ transition node to "running" state with blue pulse animation
- `node_stream` â†’ show streaming indicator on node (optional: token count)
- `node_stop` â†’ transition to "completed" (green) or "failed" (red) based on status
- `handoff` â†’ animate edge between source and target nodes

**Transitions:**
- CSS transitions for color changes (300ms ease)
- Edge animation: dashed line "flow" effect during active transfer
- Completion ripple effect on node finish

**Timing:**
- Debounce rapid updates (batch within 50ms window)
- Queue animations to prevent visual chaos during parallel execution

**State Sync:**
- Graph state synced with Execution Log scroll position
- Clicking log entry highlights corresponding node `M`

## Phase 5: Templates and Examples

58. [ ] Industry Example Library â€” Expand bundled wizard-state.json examples (3 per industry: Retail, FSI, Healthcare, Manufacturing). Improve example picker UI for 12+ demos (current UI shows 3). Examples load via existing initialization flow. `M`

59. [ ] Demo Script Generator â€” Create AI-powered talking points generator that produces demo narrative aligned with business objective and agent design `M`

## Phase 6: Enterprise Features

60. [ ] Demo Library Storage â€” Implement cloud storage for saving completed demos with metadata, tags, and search capability `L`

61. [ ] Demo Sharing â€” Add team sharing functionality with permissions and version tracking for collaborative demo development `M`

62. [ ] Demo Analytics â€” Build tracking for demo usage metrics: runs, customer reactions, conversion correlation `L`

63. [ ] Multi-Region Deployment â€” Add region selector and deployment automation for production deployments in us-east-1, us-west-2, eu-west-1 `M`

64. [ ] Demo Export â€” Create export functionality for packaging demos as standalone artifacts for offline or customer-site execution `M`

---

## Notes

- Order items by technical dependencies and product architecture
- Each item should represent an end-to-end functional and testable feature
- Single Agentify extension with two webview panels: Demo Viewer (runtime visualization) and Ideation Wizard (design-time workflow)
- **Single deployment model**: Orchestration always runs locally via `agents/main.py`, which calls agents deployed to Bedrock AgentCore. There are no separate "local" vs "AgentCore" deployment modes.
- **Dual-mode event streaming** (not deployment):
  - **stdout streaming**: Real-time JSON lines from subprocess for graph visualization
  - **DynamoDB polling**: Persistent storage for tool calls and historical replay
- **Hybrid identity**: Each run has short `workflow_id` (wf-xxx for UI/DynamoDB) + OTEL `trace_id` (32-char hex for X-Ray correlation)
- **main.py generation**: The orchestration entry point is generated by Kiro spec-driven development following patterns in `agentify-integration.md`, not created by the Agentify extension
- Strands SDK provides native OpenTelemetry support via `StrandsTelemetry` - no custom decorator package needed
- Project config stored in `.agentify/config.json`, Kiro steering in `.kiro/steering/agentify-integration.md`
- "Agentify: Initialize Project" command must run before using the extension
- CloudFormation templates in `infrastructure/` are bundled with the extension for automated deployment
- Phase 1 establishes core infrastructure before building features that depend on it
- Phase 2 AI features require Bedrock integration from earlier items
- Phase 3 Kiro integration depends on wizard outputs from Phase 2
- Phase 3.5 Conversational Workflows enables multi-turn demo execution (chat UI, session continuation, partial execution detection)
- Phase 4 Visual Polish can be deferred until after Kiro integration is working
- Phase 5-6 are enhancement phases that can be prioritized based on customer feedback
- **Agentify Power**: Bundles steering guidance and enforcement hooks into a Kiro Power package that activates on-demand during agent development
- **Enforcement Hooks**: Automatically validate generated code follows Agentify patterns (event emission, CLI contract, mock tool structure) as files are saved
- Hooks reference steering files for validation rules, creating a closed loop between documentation and enforcement

## Future Ideas

### Human-in-the-Loop with Step Functions

**Problem:** Long-running agent orchestrations may require human approval (e.g., high-value transactions, sensitive operations). Strands has an Interrupt system, but the orchestration process can't wait hours/days for approval.

**Proposed Architecture:** Step Functions as a thin durability layer around Strands orchestration:

```
Step Functions (Durability Layer)
    â”‚
    â”œâ”€â”€ Invoke Strands Orchestration (Lambda/AgentCore)
    â”‚       â”‚
    â”‚       â””â”€â”€ Strands handles all agent routing (Graph/Swarm/Workflow)
    â”‚           â”‚
    â”‚           â””â”€â”€ Agent raises interrupt â†’ returns to Step Functions
    â”‚
    â”œâ”€â”€ CheckForInterrupt (Choice state)
    â”‚       â”‚
    â”‚       â””â”€â”€ If interrupt: WaitForTaskToken (pauses indefinitely)
    â”‚
    â”œâ”€â”€ EventBridge event emitted â†’ Human notified (Slack/Email/UI)
    â”‚
    â”œâ”€â”€ Human approves â†’ EventBridge callback â†’ SendTaskSuccess(token)
    â”‚
    â””â”€â”€ Resume Strands with approval response (same session_id)
```

**Key Components:**

1. **Strands Interrupts** (`tool_context.interrupt()` or hook-based):
   - Agent detects approval needed
   - Returns `{stop_reason: "interrupt", session_id, interrupts: [...]}`

2. **Step Functions `.waitForTaskToken`**:
   - Pauses execution up to 1 year
   - Stores task token for callback resume

3. **Strands Session Persistence** (DynamoDB):
   - Saves orchestration state on interrupt
   - Resumes from exact point with approval response

4. **EventBridge Integration**:
   - Decoupled notification delivery
   - Targets: Slack, SNS, Demo Viewer UI, custom webhooks

**Benefits:**
- Strands owns all agent logic (routing, handoffs, tools)
- Step Functions provides durability (state, retries, timeouts)
- EventBridge enables flexible notification routing
- Demo Viewer can show pending approvals and approval history

**Wizard Integration (Step 4: Security & Guardrails):**
- Human Approval Gates checkboxes generate Step Functions states
- Each gate becomes a `WaitForTaskToken` checkpoint in the state machine

**Reference:** Strands Interrupts documentation: https://strandsagents.com/latest/documentation/docs/user-guide/concepts/interrupts/

---

## Technical References

- Strands Agents SDK: https://strandsagents.com/latest/
- Multi-agent patterns documentation: https://strandsagents.com/latest/documentation/docs/user-guide/concepts/multi-agent/multi-agent-patterns/
- Strands Observability/Traces: https://strandsagents.com/latest/documentation/docs/user-guide/observability-evaluation/traces/
- OpenTelemetry Context Propagation: https://opentelemetry.io/docs/concepts/context-propagation/
- W3C Trace Context (traceparent header): https://www.w3.org/TR/trace-context/
- Kiro Powers: https://kiro.dev/powers/ and https://kiro.dev/docs/powers/
- Kiro Hooks: https://kiro.dev/docs/hooks/ and https://kiro.dev/docs/hooks/types/
- Three orchestration patterns supported:
  - **Graph**: Deterministic structure with LLM-driven path selection, supports cycles, conditional edges
  - **Swarm**: Autonomous agent collaboration with emergent handoffs, supports cycles, shared context
  - **Workflow**: Fixed DAG execution with automatic parallelization, no cycles, task dependencies
