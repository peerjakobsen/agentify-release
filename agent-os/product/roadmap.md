# Product Roadmap

## Phase 1: Foundation (MVP)

1. [x] DynamoDB Observability Table — Create `agentify-workflow-events` DynamoDB table with workflow_id partition key, timestamp sort key, event_type, agent_name, payload, and TTL configuration `S`

2. [x] Agentify Extension Shell — Create single Kiro IDE extension with shared services (AWS clients, config, types) and registration for two webview panels: Demo Viewer (runtime) and Ideation Wizard (design-time) `S`

3. [x] AWS Credential Chain Integration — Use AWS SDK's default credential provider chain to automatically consume credentials from shared AWS config files (~/.aws/credentials, ~/.aws/config), supporting IAM credentials, IAM Identity Center (SSO), and assumed roles configured via AWS CLI or AWS Toolkit; add project-level region configuration in .agentify/config.json for DynamoDB and Bedrock API calls `S`

4. [x] Project Initialization Command — Add "Agentify: Initialize Project" command that: (1) checks AWS credentials via default credential chain, (2) validates DynamoDB table exists using tableValidator service, (3) if table missing, prompts user to deploy using bundled `infrastructure/dynamodb-table.yaml` template via CloudFormation SDK, (4) waits for stack CREATE_COMPLETE, (5) generates `.agentify/config.json` with table name, region, and stack name, (6) creates `.kiro/steering/agentify-integration.md` steering file. The CloudFormation template from spec #1 is packaged with the extension for automated deployment. `M`

5. [x] Workflow Input Panel — Build Demo Viewer input panel with: (1) multi-line prompt textarea (button submit only, no Enter shortcut), (2) Run Workflow button that spawns `agents/main.py` with `--prompt`, `--workflow-id`, and `--trace-id` CLI args, (3) hybrid identity display showing short `workflow_id` (wf-xxx) with copy button and OTEL `trace_id` with optional X-Ray console link, (4) execution timer, (5) validation states for missing entry script or AWS credentials. The `main.py` is generated by Kiro spec-driven development, not by this extension. `M`

6. [x] Execution Log Panel — Create chronological log panel displaying events from DynamoDB with timestamps, event types, agent names, and expandable payload details `M`

7. [x] Outcome Panel — Build outcome display section in Demo Viewer (below Execution Log) showing: (1) success/failure status with ✅/❌ icon from `workflow_complete` or `workflow_error` stdout events, (2) workflow result rendered as markdown when result is a string, with formatted JSON fallback (syntax highlighting, collapsible) for structured objects, (3) "Sources" line listing data sources used if provided in outcome payload, (4) copy-to-clipboard button for result content. Panel starts hidden/collapsed until first workflow completes, clears immediately when new run starts (not waiting for new outcome). Error state displays error message prominently without stack trace — keep it clean for demo audiences. Does NOT duplicate execution duration (already shown in Input Panel timer). `S`

8. [x] DynamoDB Polling Service — Implement polling service for workflow events: (1) poll `infrastructure.dynamodb.tableName` every 500ms using AWS SDK DocumentClient, (2) query by `workflow_id` (partition key) with `timestamp` (sort key) greater than last-polled timestamp to fetch only new events, (3) start polling when `handleRunWorkflow()` generates a workflow_id, (4) stop polling on `workflow_complete`/`workflow_error` event, panel dispose, or new workflow run, (5) exponential backoff on errors (1s, 2s, 4s, max 30s) with automatic recovery, (6) emit events to subscribers (for merging with stdout stream), (7) track seen event IDs for deduplication. Service is separate from panel lifecycle — panel subscribes to events, service manages AWS calls. `M`

9. [x] Observability Steering Documentation — Expand `.kiro/steering/agentify-integration.md` (created in item 4) with complete implementation guidance for Kiro-generated agent code:

**CLI Contract:**
- `agents/main.py` must accept `--prompt`, `--workflow-id`, `--trace-id` via argparse
- Read `AGENTIFY_TABLE_NAME` and `AGENTIFY_TABLE_REGION` environment variables
- Example argparse setup code snippet

**Hybrid Identity Pattern:**
- `workflow_id`: Short ID (wf-{8-char}) for UI display and DynamoDB partition key
- `trace_id`: 32-char hex OTEL trace ID for X-Ray correlation
- Both IDs included in every emitted event

**stdout Event Streaming (real-time, for graph visualization):**
- JSON Lines format (one JSON object per line to stdout)
- Event types: `graph_structure` (topology at start), `node_start`/`node_stream`/`node_stop` (agent lifecycle), `workflow_complete`/`workflow_error` (terminal)
- Schema: `{"workflow_id": "...", "trace_id": "...", "timestamp": 1234567890, "event_type": "...", "payload": {...}}`
- Example: mapping Strands `stream_async()` callbacks to stdout events

**DynamoDB Event Persistence (for tool calls and history):**
- Event types: `tool_call`, `tool_result`, `agent_start`, `agent_end`, `handoff`
- Write to table from `AGENTIFY_TABLE_NAME` env var using boto3
- Include TTL field for automatic cleanup
- Example: `emit_event()` helper function pattern

**Strands SDK Integration:**
- `StrandsTelemetry` setup for OTEL trace propagation
- `stream_async()` event callback mapping to stdout JSON lines
- Agent decorator patterns for consistent event emission

This steering file is the source of truth for Kiro's code generation — all generated agents must follow these patterns. `M`

10. [x] Workflow Trigger Service — Extract subprocess execution logic from `DemoViewerPanel.handleRunWorkflow()` into a dedicated service with clean separation:

**ID Generation:**
- `workflow_id`: `wf-{8-char-uuid}` (e.g., `wf-a1b2c3d4`)
- `trace_id`: 32-char hex string, OTEL-compatible (e.g., `80e1afed08e019fc1110464cfa66635c`)

**Subprocess Spawning:**
- Command: `{workflow.pythonPath} {workflow.entryScript}` from `.agentify/config.json`
- CLI args: `--prompt`, `--workflow-id`, `--trace-id`
- Env vars: `AGENTIFY_TABLE_NAME` (from `infrastructure.dynamodb.tableName`), `AGENTIFY_TABLE_REGION` (from `infrastructure.dynamodb.region`)
- Working directory: workspace root

**Event Emission (vscode.EventEmitter pattern):**
- `onStdoutLine: Event<string>` — raw stdout lines (item 11 parses these)
- `onStderr: Event<string>` — stderr output for error display
- `onProcessStateChange: Event<ProcessState>` — `idle` | `running` | `completed` | `failed` | `killed`
- `onProcessExit: Event<{code: number | null, signal: string | null}>`

**Process Lifecycle:**
- `start(prompt: string): {workflowId, traceId}` — spawns process, returns IDs
- `kill(): void` — terminates current process (for reset/new run)
- `getState(): ProcessState` — current process state
- Only one workflow at a time — calling `start()` while running kills previous process

**Integration:**
- Singleton service (like `DynamoDbPollingService`)
- `DemoViewerPanel.handleRunWorkflow()` calls this service instead of spawning directly
- Panel subscribes to events for UI updates

This service handles raw subprocess I/O; stdout JSON parsing is handled by item 11. `M`

11. [x] stdout Event Streaming & Panel Integration — Parse real-time JSON line events from `WorkflowTriggerService.onStdoutLine` following the schema in `agentify-integration.md`: `graph_structure`, `node_start`/`node_stream`/`node_stop`, `tool_call`/`tool_result`, `workflow_complete`/`workflow_error`. Create `StdoutEventParser` service that:

**Parsing:**
- Subscribes to `WorkflowTriggerService.onStdoutLine`
- Parses JSON, validates against event schema
- Emits typed `StdoutEvent` via `vscode.EventEmitter`
- Handles malformed JSON gracefully (log warning, skip)

**Panel Integration:**
- `DemoViewerPanel` subscribes to both `StdoutEventParser.onEvent` and `DynamoDbPollingService.onEvent`
- Events collected into single array, sorted by timestamp for Execution Log display
- `workflow_complete`/`workflow_error` events trigger Outcome Panel update
- `graph_structure`/`node_*` events reserved for Phase 4 Agent Graph visualization

No separate merge service needed — panel handles trivial array combination. `M`

## Phase 2: AI-Assisted Ideation

13. [x] Ideation Wizard Panel & Business Objective Step — Create Ideation Wizard webview panel with multi-step wizard navigation framework (step indicator, next/back buttons, progress tracking), then build the first step with: (1) multi-line text input for business objective/problem statement, (2) industry vertical dropdown (Retail, FSI, Healthcare, Life Sciences, Manufacturing, Energy, Telecom, Public Sector, Media & Entertainment, Travel & Hospitality, Other) with conditional "Other industry" free-text field when "Other" is selected, (3) system checkboxes grouped by category — CRM (Salesforce, HubSpot, Dynamics), ERP (SAP S/4HANA, Oracle, NetSuite), Data (Databricks, Snowflake, Redshift), HR (Workday, SuccessFactors), Service (ServiceNow, Zendesk), (4) "Other systems" free-text field, (5) optional file upload for additional context (account plan, requirements doc — stored in memory, not persisted). Wizard state held in memory; file persistence added in item 22. `M`

14. [x] Claude Bedrock Integration — Implement Amazon Bedrock client service for Claude API calls:

**Client Setup:**
- Use `@aws-sdk/client-bedrock-runtime` with credential chain from shared AWS services
- Model: `global.anthropic.claude-opus-4-5-20251101-v1:0` (configurable in `.agentify/config.json`)
- Region from `infrastructure.dynamodb.region` (same as DynamoDB)
- Use **Converse API** (`ConverseStreamCommand`) — the unified, model-agnostic interface

**Conversation Management:**
- `BedrockConversationService` singleton with `vscode.EventEmitter` pattern
- Maintain conversation history using Converse API message format:
```typescript
  interface Message {
    role: 'user' | 'assistant';
    content: Array<{ text: string }>;
  }
```
- System prompt loaded from bundled `prompts/ideation-assistant.md`, passed via `system` parameter
- `sendMessage(userMessage: string): AsyncIterable<string>` — streaming response
- `resetConversation(): void` — clear history for new ideation session

**Streaming to Webview:**
- Use `ConverseStreamCommand` from Converse API
- Async iterate `response.stream`, extract `contentBlockDelta.delta.text` tokens
- Emit tokens via `onToken: Event<string>`
- Emit `onComplete: Event<string>` with full response on `messageStop` event
- Handle `ThrottlingException` with exponential backoff (1s, 2s, 4s, max 30s)

**Error Handling:**
- `onError: Event<BedrockError>` for UI display
- Graceful handling of model access errors (user may not have Bedrock enabled)
- Handle `AccessDeniedException` for cross-region inference SCP issues `M`

15. [x] AI Gap-Filling Conversation — Create wizard step 2 as a conversational UI where the model analyzes the business objective and system selections, then proposes industry-typical assumptions:

**Conversation Flow:**
1. On step entry, auto-send context summary to Bedrock: "User's objective: {objective}. Industry: {industry}. Known systems: {systems}."
2. Model responds with structured proposal: "Based on your {industry} context, here's what I'm assuming about your environment..." with specific module/integration assumptions
3. User can accept all, or reply with corrections: "Actually we use SAP IBP, not APO"
4. Model acknowledges and refines: "Got it, updating to SAP IBP for demand planning..."
5. Conversation continues until user clicks "Confirm & Continue"

**UI Pattern:**
- Chat-style interface with AI messages (left-aligned) and user messages (right-aligned)
- AI messages include "Accept Assumptions" button for quick confirmation
- Editable text input for user refinements
- Streaming token display as model responds
- "Regenerate" button to get fresh proposal

**State Output:**
- `confirmedAssumptions: {system: string, modules: string[], integrations: string[]}[]`
- Stored in wizard state for downstream steps `L`

16. [x] Outcome Definition Step — Build wizard step 3 for defining measurable business outcomes:

**AI-Driven Suggestions on Step Entry:**
- Auto-send context (objective, industry, assumptions from Step 2) to Bedrock
- Model returns JSON with suggested primary outcome, KPIs, and relevant stakeholders
- Display suggestions as editable starting points, not locked values

**Fields:**
- Primary outcome statement (text input pre-filled with AI suggestion, fully editable)
- Success metrics (repeatable field group with AI-suggested KPIs, user can edit/add/remove)
- Stakeholders (multi-select with AI-suggested options pre-checked, user can uncheck/add custom)

**User Control:**
- All AI suggestions are editable, removable, or ignorable
- "Add Custom" option always available for each field
- "Regenerate Suggestions" button for fresh AI proposal
- User modifications take precedence over AI suggestions

**Fallback:**
- If AI fails: show static stakeholder list (Internal/External groups) and empty fields
- Manual entry always works regardless of AI status

**Validation:**
- Primary outcome required
- At least one success metric recommended (warning, not blocking) `S`

16.2. [x] Panel Architecture Consolidation — Consolidate duplicate ideation wizard implementations into single `tabbedPanel.ts`:

**Current State:**
- `src/panels/ideationWizardPanel.ts` (~3000 lines): Original standalone wizard with complete AI integration for Steps 1-3, including `OutcomeDefinitionService` integration, streaming handlers, and conversation management
- `src/panels/tabbedPanel.ts` (~2100 lines): Newer unified tabbed panel (Ideation + Demo Viewer) currently used by the app, with manual Step 3 implementation (no AI)

**Problem:**
- Two parallel implementations cause confusion during development
- AI integration code in `ideationWizardPanel.ts` is not being used
- Step 3 in `tabbedPanel.ts` lacks AI-driven suggestions that exist in `ideationWizardPanel.ts`

**Migration Tasks:**

1. **Port AI Integration for Step 3:**
   - Import and initialize `OutcomeDefinitionService` in `tabbedPanel.ts`
   - Add `_outcomeService` and `_outcomeStreamingResponse` private members
   - Implement `initOutcomeService()` method with event subscriptions
   - Implement `triggerAutoSendForStep3()` to auto-send context on step entry
   - Implement `sendOutcomeContextToClaude()` for AI suggestions
   - Add streaming handlers: `handleOutcomeStreamingToken()`, `handleOutcomeStreamingComplete()`, `handleOutcomeStreamingError()`

2. **Port Type Definitions:**
   - Verify `OutcomeSuggestions` interface from `wizardPanel.ts` is used
   - Ensure `parseOutcomeSuggestionsFromResponse()` from service is called

3. **Update Step Navigation:**
   - Add `triggerAutoSendForStep3()` call in `ideationNavigateForward()` when entering Step 3 from Step 2

4. **Port Prompt Template:**
   - Ensure `resources/prompts/outcome-definition-assistant.md` is loaded by service

5. **Delete Redundant Code:**
   - Remove `src/panels/ideationWizardPanel.ts` entirely
   - Remove `IDEATION_WIZARD_VIEW_ID` export and any package.json references
   - Clean up any unused imports in extension.ts

6. **Update Tests:**
   - Move relevant tests from `ideationWizardPanel.*.test.ts` to tabbedPanel tests
   - Delete `src/test/panels/ideationWizardPanel.*.test.ts` files

**Verification:**
- Step 3 auto-generates AI suggestions on entry (like Step 2 does)
- Regenerate button fetches fresh AI suggestions
- AI suggestions populate form fields (primary outcome, metrics, stakeholders)
- User edits are preserved and not overwritten by subsequent AI calls
- All existing Step 1-2 functionality unchanged `M`

16.5. [x] Outcome Refinement Conversation — Add conversational refinement UI to Step 3, matching the Step 2 pattern:

**Two-Phase Display:**
- **Phase 1 (Suggestion Review)**: On step entry, display AI suggestions as read-only card (not editable form)
  - Card shows: Primary Outcome statement, Suggested KPIs as bullet list, Suggested Stakeholders as tags
  - "Accept Suggestions" button (green, full-width) to transition to Phase 2
  - Refine input visible below card for pre-acceptance adjustments
- **Phase 2 (Editable Form)**: After acceptance, show current editable form (from item 16)
  - "Accepted ✓" banner at top (matches Step 2 pattern)
  - All fields now editable (textarea, metric rows, stakeholder checkboxes)
  - Refine input remains visible for post-acceptance adjustments

**Refine Input (Both Phases):**
- Text input with placeholder: "Refine outcomes..."
- "Send" button to submit refinement request
- Example hints below input: "Add a metric for cost savings", "Make the outcome more specific to risk"
- Sends natural language request to Claude with current outcome state as context
- AI responds with updated suggestions (Phase 1) or directly updates form fields (Phase 2)

**Refinement Handling:**
- Parse AI response for structured changes: outcome text updates, KPI additions/removals/edits, stakeholder changes
- In Phase 1: Update suggestion card with refined values
- In Phase 2: Update form fields directly, preserve user's other manual edits
- Show brief "Updating..." indicator while AI processes refinement

**State Management:**
- New field: `suggestionsAccepted: boolean` (false on step entry, true after Accept click)
- Preserve `suggestionsAccepted: true` when navigating back and returning to Step 3
- "Regenerate" button resets to Phase 1 (`suggestionsAccepted: false`) with fresh AI call

**Conversation Context:**
- Refinement requests include: business objective, industry, confirmed assumptions (Step 2), current outcome state
- AI maintains context for multi-turn refinements within the step
- Conversation resets on "Regenerate" or when leaving and re-entering step with fresh data

**UI Consistency with Step 2:**
- Suggestion card styling matches Step 2 assumption cards (bordered container, system headers)
- Accept button matches Step 2 green "Accepted ✓" style
- Refine input matches Step 2 "Refine assumptions..." input styling
- Loading and error states match Step 2 patterns `M`

17. [x] Security & Guardrails Step — Build wizard step 4 for compliance and approval gate configuration:

**Data Sensitivity Classification:**
- Radio buttons: Public, Internal, Confidential, Restricted
- Helper text explaining each level
- Default: Internal

**Compliance Frameworks (checkboxes):**
- SOC 2, HIPAA, PCI-DSS, GDPR, FedRAMP, None/Not specified
- Industry-aware defaults (Healthcare → HIPAA pre-checked, FSI → PCI-DSS + SOC 2)

**Human Approval Gates:**
- Checkbox list of workflow stages where human approval may be required
- Options: "Before external API calls", "Before data modification", "Before sending recommendations", "Before financial transactions"
- Default: None (fully automated demo)

**Guardrail Notes (optional text area):**
- Free-form notes for additional constraints
- On step entry: AI suggests relevant guardrail notes based on context from steps 1-2
- Suggestions are editable, user can modify or clear entirely
- Example placeholder if AI fails: "No PII in demo data, mask account numbers..."

This step is optional — "Skip" button available with sensible defaults applied. `S`

18. [x] Agent Design Proposal — Create wizard step 5 where model proposes agent team:

**State Structure:**
Add `agentDesign: AgentDesignState` to IdeationState following existing patterns:
```typescript
interface AgentDesignState {
  // AI Proposal
  proposedAgents: ProposedAgent[];
  proposedOrchestration: OrchestrationPattern;
  proposedEdges: ProposedEdge[];
  orchestrationReasoning: string;

  // Accept/Edit State
  proposalAccepted: boolean;
  isLoading: boolean;
  error?: string;

  // Change Detection
  step4Hash?: string;
  aiCalled: boolean;
}

interface ProposedAgent {
  id: string;
  name: string;
  role: string;
  tools: string[];  // AI-generated from Step 1 systems
}

interface ProposedEdge {
  from: string;
  to: string;
  condition?: string;  // For graph pattern
}
```

**Auto-Proposal on Step Entry:**
- Trigger: `triggerAutoSendForStep5()` following Step 3 pattern
- Change detection: Hash of Steps 1-4 inputs
- Send context to Bedrock, request JSON-structured agent team
- Parse response, populate `proposed*` fields

**Display (Phase 1 — Before Accept):**
- Card grid: each agent shows name, role, tools as tags
- Orchestration badge: "Graph" / "Swarm" / "Workflow"
- "Why this pattern?" expandable with `orchestrationReasoning`
- Text-based flow summary (NOT visual diagram):
```
  Flow: Planner → Recommender → Output
```

**Actions:**
- "↻ Regenerate" — `handleRegenerateAgentProposal()`, clears and re-fetches
- "Accept & Continue" — sets `proposalAccepted: true`, proceeds to Step 6
- "Let me adjust..." — sets `proposalAccepted: true`, stays on step, shows edit UI (item 19)

**Tool Generation:**
- AI generates tool names based on systems from Step 1
- Format: `{system}_{operation}` (e.g., `sap_get_inventory`, `salesforce_query_accounts`)
- Editable in item 19 `M`

19. [x] Agent Design Refinement — Enable editing when "Let me adjust..." selected:

**Transition:**
- Same page, different UI mode (like Step 3's Phase 1 → Phase 2)
- Show "Accepted ✓" banner (following Step 3 pattern)

**Agent Card Editing:**
- Each agent as editable card with edited flags:
  - `nameEdited`, `roleEdited`, `toolsEdited` per agent
- Fields: Name (text), Role (textarea), Tools (tag input with × remove)
- "× Remove Agent" with confirmation if agent has edges
- "+ Add Agent" opens card with empty fields

**Orchestration Adjustment:**
- Dropdown: graph / swarm / workflow
- Shows AI recommendation badge on original suggestion
- On change: AI suggests updated edges (non-blocking suggestion)

**Edge Editing:**
- Simple table: "From" dropdown → "To" dropdown
- Add/remove edge buttons
- Validation warnings (non-blocking):
  - Orphan agents (no connections)
  - No entry point

**AI Assistance (Optional — Deferred):**
- "✨ Suggest tools" button per agent → quick AI call
- "Validate Design" button → AI reviews, shows suggestions in toast

**Confirm:**
- "Confirm Design" copies to `confirmed*` fields, proceeds to Step 6
- Edited flags prevent AI overwrite on back-navigation `L`

20. [x] Orchestration Pattern Help — Tooltips on dropdown options explaining each pattern:

**Implementation:**
- Added `title` attributes to orchestration dropdown `<option>` elements
- Hover over any option (graph/swarm/workflow) to see description

**Pattern Descriptions:**
- **graph**: "LLM picks path at runtime based on conditions. Best for: approval gates, decision trees, conditional workflows."
- **swarm**: "Agents hand off autonomously. Best for: complex problem-solving, collaborative analysis, emergent behavior."
- **workflow**: "Fixed DAG with parallel execution. Best for: predictable pipelines, batch processing, strict ordering."

**Deferred:**
- Edge condition labels (Graph-specific)
- Handoff limits (Swarm-specific)
- Parallel group config (Workflow-specific)

21. [x] Mock Data Strategy — Build wizard step 6 for AI-generated mock data configuration:

**Auto-Generation on Step Entry:**
- For each tool identified in agent design, model proposes mock data shape:
```json
{
  "tool": "sap_inventory",
  "system": "SAP S/4HANA",
  "operation": "get_stock_levels",
  "mockRequest": {"warehouse_id": "string", "sku_list": "string[]"},
  "mockResponse": {"sku": "string", "quantity": "number", "location": "string"},
  "sampleData": [
    {"sku": "TOMATO-001", "quantity": 150, "location": "Produce-A3"}
  ]
}
```

**Display & Editing:**
- Accordion for each tool with mock definition
- JSON editor for request/response schemas (with syntax highlighting)
- Sample data table with add/edit/delete rows
- "Use customer terminology" toggle: when ON, model regenerates with industry-specific naming from wizard context

**Bulk Actions:**
- "Regenerate All" — fresh mock data proposal from model
- "Import Sample Data" — upload CSV/JSON to populate sample data tables

**Validation:**
- Warn if any tool missing mock definition
- Warn if sample data empty (demo won't be realistic)

**Output:**
- Mock definitions stored in wizard state
- Used in Phase 4 steering file generation (`integration-landscape.md`) `M`

22. [x] Wizard State Persistence — Implement workspace storage for wizard progress so users can resume incomplete ideation sessions:

**Storage:**
- Save wizard state to `.agentify/wizard-state.json` on each step completion
- State includes: current step, all field values, conversation history, agent design, mock data config
- Exclude uploaded files (too large) — store file metadata only with "re-upload required" flag

**Resume Flow:**
- On Ideation Wizard open, check for existing `wizard-state.json`
- If found and less than 7 days old: prompt "Resume previous session?" with preview of business objective
- "Resume" → restore state, navigate to last completed step
- "Start Fresh" → delete state file, begin at step 1

**Auto-Save:**
- Debounced save (500ms after last change) within each step
- Explicit save on "Next" button click

**Clear State:**
- "Reset Wizard" command clears state file and restarts
- State automatically cleared when steering files successfully generated (Phase 4) `S`

23. [x] Demo Design Step — Build wizard step 7 for capturing demo presentation strategy:

**Key "Aha Moments" (repeatable field group):**
- Moment title: "What should impress the audience?" — e.g., "Real-time SAP inventory sync"
- When it occurs: dropdown selecting which agent/tool triggers this moment
- What to say: suggested talking point for presenter

**Demo Persona:**
- Persona name (text): e.g., "Maria, Regional Inventory Manager"
- Persona role (text): e.g., "Reviews morning replenishment recommendations for 12 stores"
- Persona pain point (text): e.g., "Currently spends 2 hours manually checking stock levels"
- AI assist: "Generate Persona" button creates persona based on industry/objective context

**Narrative Flow:**
- Ordered list of demo scenes (drag-to-reorder)
- Each scene: title, description, which agents are highlighted
- "Generate Narrative" button: model proposes scene sequence based on agent design

**Output:**
- Stored in wizard state for `demo-strategy.md` generation in Phase 4 `M`

24. [ ] Generate Step (Wizard Step 8) — Build the final wizard step that orchestrates steering file generation and Kiro handoff:

**Pre-Generation Checklist Display:**
- Show read-only summary of all wizard inputs across steps 1-7
- Validation status for each step (green check if complete, warning if optional fields skipped)
- "Edit" button next to each section to jump back to that step

**Generation Progress UI:**
- Checklist with real-time status updates:
  - [ ] Validate wizard inputs
  - [ ] Generate steering files (→ Phase 4, Item 28)
  - [ ] Ready for Kiro
- Each item shows spinner while in progress, checkmark on success, X on failure
- Error details expandable if any step fails
- Note: Agentify Power is installed during project initialization, not here

**Actions:**
- "Generate" button — triggers the generation sequence
- "Generate & Open in Kiro" button — generates then triggers Kiro spec flow (→ Phase 4, Item 34)
- Progress is non-blocking — user can see what's happening

**Post-Generation:**
- Success state shows generated file list with "Open File" links
- "Start Over" button to begin new ideation session (clears wizard state)
- If not in Kiro IDE, show message directing user to open project in Kiro

**Dependencies:**
- Requires Phase 3 Item 28 (steering generation) and Item 34 (Kiro trigger) for full functionality
- Agentify Power (Items 29-33) is installed at project init, not here
- Can show placeholder/disabled state until Phase 3 is complete `M`

## Phase 3: Kiro Integration & Enforcement

**Implementation Context:**
- **Demo Scope:** This entire extension generates demos with mock integrations - no real system integrations (SAP, Salesforce, etc.)
- **Local Entry:** `main.py` runs locally, triggers the agent workflow
- **Agent Runtime:** Strands agents deploy to AgentCore Runtime via **AgentCore CLI**
- **Tools:** Either inline with agent code OR as Lambda functions (for shared tools across agents)
- **Orchestration:** Steering files describe the pattern (graph/swarm/workflow), agent definitions, edges, and entry point(s)

28. [ ] Kiro Steering Generation — Generate complete `.kiro/steering/` directory from wizard state on "Generate Steering Files" button click:

**Files Generated:**

| File | Source | Content |
|------|--------|---------|
| `product.md` | Item 13, 16 | Business objective as product description, success metrics, stakeholders |
| `tech.md` | Item 17, 18, 19, 20 | Strands SDK, Python 3.12+, orchestration pattern, agent definitions with roles/tools, edges defining data flow, entry point agent(s), **AgentCore features mapping** (see below), DynamoDB for observability events |
| `structure.md` | Static template | Standard agentic project layout (`agents/`, `tools/`, `mocks/`, etc.) |
| `customer-context.md` | Item 13, 15 | Industry, confirmed system assumptions, strategic priorities |
| `integration-landscape.md` | Item 15, 19, 21 | Systems, data sources, mock definitions with sample data, **shared tools analysis** (see below) |
| `security-policies.md` | Item 17 | Data classification, compliance frameworks, approval gates, guardrails |
| `demo-strategy.md` | Item 23 | Key aha moments, demo persona, narrative flow sequence |
| `agentify-integration.md` | Static + Item 18 | Event emission patterns, CLI contract, agent IDs from design |

**AgentCore Features Mapping (in `tech.md`):**

Steering docs should guide Kiro on which AgentCore features to use:

| AgentCore Feature | Demo Usage | Notes |
|-------------------|------------|-------|
| **Runtime** | ✅ Deploy agents via AgentCore CLI | Serverless, session isolation |
| **Gateway** | ✅ Register shared Lambda tools | Auto-converts to MCP-compatible |
| **Memory** | Optional | If agents need session context |
| **Identity** | Skip | Mock tools don't need real auth |
| **Policy** | ✅ Map Step 4 guardrails to Cedar policies | Approval gates → boundaries |
| **Evaluations** | Optional | Test demo quality |
| **Observability** | ✅ Agentify DynamoDB events | Powers the Agentify panel UI |

**Policy Mapping from Step 4:**
```markdown
## AgentCore Policy Configuration
Based on Security & Guardrails (Step 4):

Approval Gates → Cedar Policies:
- "Before external API calls" → require_approval(action == "external_api")
- "Before data modification" → require_approval(action == "write")
- "Before financial transactions" → require_approval(action == "financial")

Data Sensitivity → Access Boundaries:
- "Confidential" → restrict PII fields, audit all access
- "Restricted" → encryption required, no external transmission
```

**Shared Tools Analysis (in `integration-landscape.md`):**

When generating steering docs, analyze tools across all agents to identify shared vs per-agent tools:

- **Identify duplicates:** If multiple agents use the same system integration (e.g., `databricks_query`, `salesforce_get_customer`), flag as shared tool
- **Shared tools:** Deploy as Lambda functions, register with Gateway (auto-converts to MCP-compatible)
- **Per-agent tools:** Inline with agent code
- **Output format in steering doc:**
```markdown
## Shared Tools (Lambda functions, registered with Gateway)
| Tool | Used By | Mock Data Source |
|------|---------|------------------|
| databricks_query | Credit Agent, Risk Agent, Compliance Agent | Item 21 mock definitions |
| salesforce_get_customer | Document Agent, Credit Agent | Item 21 mock definitions |

## Per-Agent Tools (inline with agent code)
| Tool | Agent | Notes |
|------|-------|-------|
| fraud_score_calculator | Fraud Agent | Agent-specific logic |
```

**Generation Flow:**
1. Validate all required wizard steps completed
2. Show preview of files to be generated
3. User confirms "Generate"
4. Write files to `.kiro/steering/`
5. Clear wizard state (item 22) on success
6. Show success notification with "Open in Kiro" button

**Conflict Handling:**
- If `.kiro/steering/` exists, prompt: "Overwrite existing steering files?"
- Option to backup existing files to `.kiro/steering.backup/` `M`

29. [ ] Agentify Power Package — Create Kiro Power that bundles steering guidance and enforcement hooks. **This is a generic package installed during project initialization (extends Item 4), not per-ideation.** Ensures all agent code follows Agentify patterns from day one:

**Power Structure:**
```
agentify-power/
├── POWER.md              # Steering for agentic workflow development
└── hooks/
    ├── observability-enforcer.kiro.hook
    ├── cli-contract-validator.kiro.hook
    └── mock-tool-pattern.kiro.hook
```

**POWER.md Content:**
- Best practices for Strands agent development
- Event emission patterns (reference `agentify-integration.md`)
- CLI contract requirements (Agentify CLI for local testing)
- AgentCore CLI deployment patterns (deploy to Bedrock AgentCore Runtime)
- Mock tool implementation guidelines (demo scope)
- Common pitfalls and solutions

**Activation Keywords:**
- "agent", "workflow", "Strands", "orchestrator", "demo", "multi-agent"

**Distribution:**
- Bundled with Agentify extension in `resources/agentify-power/`
- Can be published to Kiro community powers for standalone use `M`

30. [ ] Observability Enforcement Hook — Create `observability-enforcer.kiro.hook`:

**Trigger:**
- Event: `fileSaved`
- Pattern: `agents/*.py`

**Validation Rules:**
- Check for `emit_stdout_event()` or equivalent calls in agent functions
- Verify `workflow_id` and `trace_id` passed to all event emissions
- Check `graph_structure` emitted before `node_start` events
- Verify terminal event (`workflow_complete` or `workflow_error`) emitted

**Hook Prompt:**
```
Review this agent file for Agentify observability compliance.
Reference: .kiro/steering/agentify-integration.md

Check:
1. All agent functions emit node_start/node_stop events
2. Tool calls emit tool_call events to DynamoDB
3. workflow_id and trace_id included in all events
4. Terminal event emitted on completion/error

Suggest fixes for any missing observability code.
```

**Output:**
- Inline suggestions for missing event emissions
- Warning if no observability code detected `S`

31. [ ] CLI Contract Validation Hook — Create `cli-contract-validator.kiro.hook`:

**Trigger:**
- Event: `fileSaved`
- Pattern: `agents/main.py`

**Validation Rules:**
- Check `argparse` setup exists
- Verify `--prompt` argument defined and required
- Verify `--workflow-id` argument defined and required
- Verify `--trace-id` argument defined and required
- Check `os.environ.get('AGENTIFY_TABLE_NAME')` present
- Check `os.environ.get('AGENTIFY_TABLE_REGION')` present

**Hook Prompt:**
Validate this main.py entry point against Agentify CLI contract.
Reference: .kiro/steering/agentify-integration.md Section 1
Required CLI arguments: --prompt, --workflow-id, --trace-id
Required env vars: AGENTIFY_TABLE_NAME, AGENTIFY_TABLE_REGION
Flag any missing arguments or environment variable reads.
Suggest argparse setup if not present.
`````S`

32. [ ] Mock Tool Pattern Hook — Create mock-tool-pattern.kiro.hook (validates mock integrations for demo purposes, not real integrations):

Trigger:

Event: fileCreated
Pattern: tools/*.py

Validation Rules:

Check for Strands @tool decorator on functions
Verify function has docstring (used by Strands for tool description)
Check return type annotation present
Validate mock data structure if integration-landscape.md exists

Hook Prompt:
Review this new tool file for Strands SDK compliance.
Reference: .kiro/steering/integration-landscape.md for expected mock data shapes

Check:
1. @tool decorator present on tool functions
2. Docstring describes what the tool does
3. Type hints on parameters and return value
4. Mock response matches schema in integration-landscape.md

Suggest improvements for realistic mock behavior.
`````S`

33. [ ] Power Installation Integration — **Extend Project Initialization (Item 4)** to install Agentify Power during "Agentify: Initialize Project" command:

**Installation Flow:**
1. After writing steering files, check if Agentify Power installed
2. If not installed, copy from `resources/agentify-power/` to `.kiro/powers/agentify/`
3. Register power in `.kiro/powers/manifest.json`

**Power Manifest:**
```json
{
  "powers": [
    {
      "name": "agentify",
      "path": "./agentify",
      "activationKeywords": ["agent", "workflow", "Strands", "orchestrator", "demo"]
    }
  ]
}
```

**Verification:**
- Validate hooks are syntactically correct
- Show notification: "Agentify Power installed - enforcement hooks active" `S`

34. [ ] Kiro Spec Trigger — Implement seamless handoff from Ideation Wizard to Kiro spec mode:

**Pre-Flight Checks:**
1. Verify all steering files exist in `.kiro/steering/`
2. Verify Agentify Power installed (item 33)
3. Verify `.agentify/config.json` exists with valid DynamoDB config

**Trigger Flow:**
1. User clicks "Generate Code with Kiro" button in wizard
2. Run pre-flight checks, show errors if any fail
3. Build initial prompt from wizard context:
```
   Create a multi-agent workflow based on the steering files in .kiro/steering/.

   Business objective: {from product.md}
   Orchestration pattern: {from tech.md}
   Agents: {from agent design}

   Start with agents/main.py following the CLI contract in agentify-integration.md.
```
4. Execute Kiro command: `kiro.startSpecFlow` with prompt
5. Show confirmation: "Kiro spec mode started. Enforcement hooks are active."

**VS Code Fallback:**
- Detect if running in VS Code (not Kiro) via `vscode.env.appName`
- Show message: "Code generation requires Kiro IDE. Steering files have been generated in .kiro/steering/. Open this project in Kiro to continue."
- Offer "Learn More" link to Kiro download page `S`

## Phase 4: Visual Polish

25. [ ] Agent Graph Visualization — Add React Flow visualization to Demo Viewer with custom node components showing agent status (pending/running/completed/failed), animated edges during data flow, auto-layout via dagre/elkjs, and pattern-specific layouts:

**Node Components:**
- Custom React Flow node for each agent
- Status indicator: gray (pending), blue pulse (running), green (completed), red (failed)
- Agent name and role displayed
- Tool call count badge

**Edge Styling:**
- Animated dashes during active data flow
- Edge labels for Graph pattern conditions
- Bidirectional arrows for Swarm handoffs

**Layout Algorithms:**
- **Graph**: Dagre top-to-bottom DAG layout with conditional edge routing
- **Swarm**: Force-directed peer-to-peer layout (circular for small graphs)
- **Workflow**: ELKjs layered layout with parallel execution lanes

**Initialization:**
- Read `graph_structure` event from stdout to build initial topology
- Fall back to agent design from `.agentify/config.json` if no event received

**Interaction:**
- Click node to highlight in Execution Log
- Zoom/pan controls
- "Fit to view" button `L`

26. [ ] Graph Animation — Implement real-time graph updates from stdout events with smooth transitions:

**Event Handling:**
- `node_start` → transition node to "running" state with blue pulse animation
- `node_stream` → show streaming indicator on node (optional: token count)
- `node_stop` → transition to "completed" (green) or "failed" (red) based on status
- `handoff` → animate edge between source and target nodes

**Transitions:**
- CSS transitions for color changes (300ms ease)
- Edge animation: dashed line "flow" effect during active transfer
- Completion ripple effect on node finish

**Timing:**
- Debounce rapid updates (batch within 50ms window)
- Queue animations to prevent visual chaos during parallel execution

**State Sync:**
- Graph state synced with Execution Log scroll position
- Clicking log entry highlights corresponding node `M`

27. [ ] Enhanced Log Formatting — Improve Execution Log panel with advanced formatting and filtering:

**Collapsible Sections:**
- Group events by agent (collapsible agent sections)
- Tool calls collapsed by default, expandable to show input/output
- "Expand All" / "Collapse All" toolbar buttons

**Syntax Highlighting:**
- JSON payloads with syntax highlighting (use existing `tokenizeJson()`)
- SQL queries highlighted if detected in tool input
- Markdown rendering for text content

**Filtering:**
- Filter dropdown: "All Events", "Agent Events Only", "Tool Calls Only", "Errors Only"
- Agent filter: multi-select to show only specific agents
- Search box: text search across event content

**Performance:**
- Virtual scrolling for large event lists (>100 events)
- Lazy render expanded payload content `M`

## Phase 5: Templates and Patterns

38. [ ] Industry Template Framework — Build template system for storing and loading pre-built agent patterns with metadata `M`

39. [ ] Retail Industry Template — Create agent patterns for common retail scenarios: inventory optimization, customer service, demand forecasting `M`

40. [ ] FSI Industry Template — Create agent patterns for financial services: fraud detection, customer onboarding, risk assessment `M`

41. [ ] Healthcare Industry Template — Create agent patterns for healthcare: patient scheduling, claims processing, clinical decision support `M`

42. [ ] Manufacturing Industry Template — Create agent patterns for manufacturing: predictive maintenance, quality control, supply chain optimization `M`

43. [ ] Value Map Template Framework — Build storage and loading system for value map templates with metadata schema including recommended orchestration pattern `M`

44. [ ] Common Value Map Templates — Create templates for common value maps, each with suggested agent teams and recommended Strands pattern: Cost Reduction (typically Workflow for deterministic optimization pipeline), Revenue Growth (typically Graph for conditional customer journey routing), Operational Efficiency (typically Workflow for parallel automation tasks), Customer Experience (typically Swarm for collaborative issue resolution), Risk Mitigation (typically Graph for decision trees with approval gates) `L`

45. [ ] Demo Script Generator — Create AI-powered talking points generator that produces demo narrative aligned with business objective and agent design `M`

## Phase 6: Enterprise Features

46. [ ] Demo Library Storage — Implement cloud storage for saving completed demos with metadata, tags, and search capability `L`

47. [ ] Demo Sharing — Add team sharing functionality with permissions and version tracking for collaborative demo development `M`

48. [ ] Demo Analytics — Build tracking for demo usage metrics: runs, customer reactions, conversion correlation `L`

49. [ ] Multi-Region Deployment — Add region selector and deployment automation for production deployments in us-east-1, us-west-2, eu-west-1 `M`

50. [ ] Demo Export — Create export functionality for packaging demos as standalone artifacts for offline or customer-site execution `M`

---

## Notes

- Order items by technical dependencies and product architecture
- Each item should represent an end-to-end functional and testable feature
- Single Agentify extension with two webview panels: Demo Viewer (runtime visualization) and Ideation Wizard (design-time workflow)
- **Single deployment model**: Orchestration always runs locally via `agents/main.py`, which calls agents deployed to Bedrock AgentCore. There are no separate "local" vs "AgentCore" deployment modes.
- **Dual-mode event streaming** (not deployment):
  - **stdout streaming**: Real-time JSON lines from subprocess for graph visualization
  - **DynamoDB polling**: Persistent storage for tool calls and historical replay
- **Hybrid identity**: Each run has short `workflow_id` (wf-xxx for UI/DynamoDB) + OTEL `trace_id` (32-char hex for X-Ray correlation)
- **main.py generation**: The orchestration entry point is generated by Kiro spec-driven development following patterns in `agentify-integration.md`, not created by the Agentify extension
- Strands SDK provides native OpenTelemetry support via `StrandsTelemetry` - no custom decorator package needed
- Project config stored in `.agentify/config.json`, Kiro steering in `.kiro/steering/agentify-integration.md`
- "Agentify: Initialize Project" command must run before using the extension
- CloudFormation templates in `infrastructure/` are bundled with the extension for automated deployment
- Phase 1 establishes core infrastructure before building features that depend on it
- Phase 2 AI features require Bedrock integration from earlier items
- Phase 3 Kiro integration depends on wizard outputs from Phase 2
- Phase 4 Visual Polish can be deferred until after Kiro integration is working
- Phase 5-6 are enhancement phases that can be prioritized based on customer feedback
- **Agentify Power**: Bundles steering guidance and enforcement hooks into a Kiro Power package that activates on-demand during agent development
- **Enforcement Hooks**: Automatically validate generated code follows Agentify patterns (event emission, CLI contract, mock tool structure) as files are saved
- Hooks reference steering files for validation rules, creating a closed loop between documentation and enforcement

## Technical References

- Strands Agents SDK: https://strandsagents.com/latest/
- Multi-agent patterns documentation: https://strandsagents.com/latest/documentation/docs/user-guide/concepts/multi-agent/multi-agent-patterns/
- Strands Observability/Traces: https://strandsagents.com/latest/documentation/docs/user-guide/observability-evaluation/traces/
- OpenTelemetry Context Propagation: https://opentelemetry.io/docs/concepts/context-propagation/
- W3C Trace Context (traceparent header): https://www.w3.org/TR/trace-context/
- Kiro Powers: https://kiro.dev/powers/ and https://kiro.dev/docs/powers/
- Kiro Hooks: https://kiro.dev/docs/hooks/ and https://kiro.dev/docs/hooks/types/
- Three orchestration patterns supported:
  - **Graph**: Deterministic structure with LLM-driven path selection, supports cycles, conditional edges
  - **Swarm**: Autonomous agent collaboration with emergent handoffs, supports cycles, shared context
  - **Workflow**: Fixed DAG execution with automatic parallelization, no cycles, task dependencies
